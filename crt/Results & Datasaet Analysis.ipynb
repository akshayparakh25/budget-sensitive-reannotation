{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import textstat as txt\n",
    "from itertools import groupby\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading TACRED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('./../dataset/tacred/train.json'))\n",
    "print(\"Number of Training instances :: {}\".format(len(train_data)))\n",
    "\n",
    "dev_data = json.load(open('./../dataset/tacred/dev.json'))\n",
    "print(\"Number of Dev instances :: {}\".format(len(dev_data)))\n",
    "\n",
    "test_data = json.load(open('./../dataset/tacred/test.json'))\n",
    "print(\"Number of Test instances :: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_length_mapping(dataset):\n",
    "    s_ids = []\n",
    "    s_lens = []\n",
    "    for eg in dataset:\n",
    "        s_id = eg['id']\n",
    "        tokens = eg['token']\n",
    "        s_ids.append(s_id)\n",
    "        s_lens.append(len(tokens))\n",
    "    \n",
    "    assert len(s_ids) == len(s_lens) \n",
    "    print(len(s_ids), len(s_lens) )\n",
    "    df = pd.DataFrame({'sentence_id':s_ids, 'sentence_len':s_lens})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_len = generate_sentence_length_mapping(test_data)\n",
    "df_dev_len = generate_sentence_length_mapping(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token(token):\n",
    "    \"\"\" Convert PTB tokens to normal tokens \"\"\"\n",
    "    if (token.lower() == '-lrb-'):\n",
    "            return '('\n",
    "    elif (token.lower() == '-rrb-'):\n",
    "        return ')'\n",
    "    elif (token.lower() == '-lsb-'):\n",
    "        return '['\n",
    "    elif (token.lower() == '-rsb-'):\n",
    "        return ']'\n",
    "    elif (token.lower() == '-lcb-'):\n",
    "        return '{'\n",
    "    elif (token.lower() == '-rcb-'):\n",
    "        return '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch-Kincaid Grade readability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flesch_kincaid_grade(dataset):\n",
    "    s_ids = []\n",
    "    score = []\n",
    "    for i, eg in enumerate(dataset, start=1):\n",
    "        s_id = eg['id']\n",
    "        tokens  = eg['token']\n",
    "        sentence = ' '.join([convert_token(t) for t in tokens])\n",
    "        s_ids.append(s_id)\n",
    "        score.append(txt.flesch_kincaid_grade(sentence))\n",
    "    assert len(s_ids) == len(score)\n",
    "    print(len(s_ids), len(score))\n",
    "    df = pd.DataFrame({'sentence_id':s_ids, 'f_k_score':score})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fk = generate_flesch_kincaid_grade(test_data)\n",
    "df_dev_fk = generate_flesch_kincaid_grade(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_specific_attribute(dataset):\n",
    "    s_ids = []\n",
    "    distance = []\n",
    "    e_count = []\n",
    "    for eg in dataset:\n",
    "        s_id = eg['id']\n",
    "        s_ids.append(s_id)\n",
    "        s_start = eg['subj_start']\n",
    "        s_end = eg['subj_end']\n",
    "        o_start = eg['obj_start']\n",
    "        o_end = eg['obj_end']\n",
    "        s_type = eg['subj_type']\n",
    "        o_type = eg['obj_type']\n",
    "        subj = eg['token'][s_start:s_end+1]\n",
    "        obj = eg['token'][o_start:o_end+1]\n",
    "        # number of tokens between the subject and object entities\n",
    "        if s_end < o_start:\n",
    "            e_dist = o_start - s_end - 1\n",
    "        elif o_end < s_start:\n",
    "            e_dist = s_start - o_end - 1\n",
    "        distance.append(e_dist)\n",
    "\n",
    "        # Number of entities in the sentence based on stanford NER\n",
    "        ner = eg['stanford_ner']\n",
    "        s = e = 0\n",
    "        n_e = 0\n",
    "        p_et = None\n",
    "        for i, et in enumerate(ner):\n",
    "            if et == 'O':\n",
    "                if e != 0:\n",
    "                    n_e += 1\n",
    "                    s = e = 0\n",
    "                    p_et = None\n",
    "                continue\n",
    "            else:\n",
    "                if p_et and p_et != et:\n",
    "                    n_e += 1\n",
    "                    s = e = 0\n",
    "                    p_et = None\n",
    "                s += 1\n",
    "                e += 1\n",
    "            p_et = et\n",
    "        e_count.append(n_e)\n",
    "    assert len(s_ids) == len(distance)\n",
    "    assert len(s_ids) == len(e_count)\n",
    "    print(len(s_ids), len(distance), len(e_count))\n",
    "    df = pd.DataFrame({'sentence_id':s_ids, 'entities_distance':distance, 'entities_count':e_count})\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ef = generate_entity_specific_attribute(test_data)\n",
    "df_dev_ef = generate_entity_specific_attribute(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TACREV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacrev_test = json.load(open('./tacrev_patch/test_patch.json'))\n",
    "\n",
    "sentence_id = []\n",
    "for item in tacrev_test:\n",
    "    sentence_id.append(item['id'])\n",
    "    \n",
    "df_test = pd.DataFrame({'sentence_id':sentence_id})\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacrev_dev = json.load(open('./tacrev_patch/dev_patch.json'))\n",
    "\n",
    "sentence_id = []\n",
    "for item in tacrev_dev:\n",
    "    sentence_id.append(item['id'])\n",
    "    \n",
    "df_dev = pd.DataFrame({'sentence_id':sentence_id})\n",
    "# df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2))) / float(len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_test.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'confidence']]\n",
    "df1 = df1.rename(columns={'confidence':'confidence_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_test.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'confidence']]\n",
    "df2 = df2.rename(columns={'confidence': 'confidence_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_test.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'confidence']]\n",
    "df3 = df3.rename(columns={'confidence': 'confidence_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "dfconf = df1.merge(df2).merge(df3)\n",
    "dfconf['confidence'] = (dfconf['confidence_parnn'] + dfconf['confidence_lstm'] + dfconf['confidence_gcn']) / 3\n",
    "dfconf = dfconf.merge(df_test_len)\n",
    "dfconf = dfconf.merge(df_test_fk)\n",
    "dfconf = dfconf.merge(df_test_ef)\n",
    "print('\\n')\n",
    "print(len(dfconf))\n",
    "dfconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf = dfconf.sort_values(by=['confidence'], ascending=False)\n",
    "sorted_dfconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_c = []\n",
    "for i in range(len(sorted_dfconf)):\n",
    "    n_dp = len(df_test.merge(sorted_dfconf[:i]))\n",
    "    freq_c.append(n_dp)\n",
    "print(len(freq_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_c = []\n",
    "for i in range(5,len(sorted_dfconf)):\n",
    "    s = list(df_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    c = list(df_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    intersection_c.append(jaccard_similarity(s, c)*100)\n",
    "print(len(intersection_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.merge(sorted_dfconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence for dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_dev.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'confidence']]\n",
    "df1 = df1.rename(columns={'confidence':'confidence_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_dev.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'confidence']]\n",
    "df2 = df2.rename(columns={'confidence': 'confidence_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_dev.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'confidence']]\n",
    "df3 = df3.rename(columns={'confidence': 'confidence_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "dfconf_dev = df1.merge(df2).merge(df3)\n",
    "dfconf_dev['confidence'] = (dfconf_dev['confidence_parnn'] + dfconf_dev['confidence_lstm'] + dfconf_dev['confidence_gcn']) / 3\n",
    "dfconf_dev = dfconf_dev.merge(df_dev_len)\n",
    "dfconf_dev = dfconf_dev.merge(df_dev_fk)\n",
    "dfconf_dev = dfconf_dev.merge(df_dev_ef)\n",
    "print('\\n')\n",
    "print(len(dfconf_dev))\n",
    "dfconf_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf_dev = dfconf_dev.sort_values(by=['confidence'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_freq_c = []\n",
    "for i in range(len(sorted_dfconf_dev)):\n",
    "    n_dp = len(df_dev.merge(sorted_dfconf_dev[:i]))\n",
    "    dev_freq_c.append(n_dp)\n",
    "print(len(dev_freq_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_intersection_c = []\n",
    "for i in range(5, len(sorted_dfconf_dev)):\n",
    "    s = list(df_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    c = list(df_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "#     print(s,c)\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    dev_intersection_c.append(jaccard_similarity(s, c)*100)\n",
    "print(len(dev_intersection_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.merge(sorted_dfconf_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction distance from ground truth on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_test.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_prediction']]\n",
    "df1 = df1.rename(columns={'d_prediction':'d_prediction_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_test.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_prediction']]\n",
    "df2 = df2.rename(columns={'d_prediction': 'd_prediction_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_test.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_prediction']]\n",
    "df3 = df3.rename(columns={'d_prediction': 'd_prediction_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "df_pd = df1.merge(df2).merge(df3)\n",
    "df_pd['d_prediction'] = (df_pd['d_prediction_parnn'] + df_pd['d_prediction_lstm'] + df_pd['d_prediction_gcn']) / 3\n",
    "df_pd = df_pd.merge(df_test_len)\n",
    "df_pd = df_pd.merge(df_test_fk)\n",
    "df_pd = df_pd.merge(df_test_ef)\n",
    "print('\\n')\n",
    "print(len(df_pd))\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_pd = df_pd.sort_values(by=['d_prediction'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_p = []\n",
    "for i in range(len(sorted_df_pd)):\n",
    "    n_dp = len(df_test.merge(sorted_df_pd[:i]))\n",
    "    freq_p.append(n_dp)\n",
    "print(len(freq_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_p = []\n",
    "for i in range(5,len(sorted_df_pd)):\n",
    "    s = list(df_test.merge(sorted_df_pd[:i])['sentence_id'])\n",
    "    c = list(df_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "#     print(s,c)\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    intersection_p.append(jaccard_similarity(s, c)*100)\n",
    "print(len(intersection_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.merge(sorted_df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction distance from ground truth on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_dev.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_prediction']]\n",
    "df1 = df1.rename(columns={'d_prediction':'d_prediction_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_dev.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_prediction']]\n",
    "df2 = df2.rename(columns={'d_prediction': 'd_prediction_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_dev.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_prediction']]\n",
    "df3 = df3.rename(columns={'d_prediction': 'd_prediction_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "df_pd_dev = df1.merge(df2).merge(df3)\n",
    "df_pd_dev['d_prediction'] = (df_pd_dev['d_prediction_parnn'] + df_pd_dev['d_prediction_lstm'] + df_pd_dev['d_prediction_gcn']) / 3\n",
    "df_pd_dev = df_pd_dev.merge(df_dev_len)\n",
    "df_pd_dev = df_pd_dev.merge(df_dev_fk)\n",
    "df_pd_dev = df_pd_dev.merge(df_dev_ef)\n",
    "print('\\n')\n",
    "print(len(df_pd_dev))\n",
    "df_pd_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_pd_dev = df_pd_dev.sort_values(by=['d_prediction'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_freq_p = []\n",
    "for i in range(len(sorted_df_pd_dev)):\n",
    "    n_dp = len(df_dev.merge(sorted_df_pd_dev[:i]))\n",
    "    dev_freq_p.append(n_dp)\n",
    "print(len(dev_freq_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_intersection_p = []\n",
    "for i in range(5, len(sorted_df_pd_dev)):\n",
    "    s = list(df_dev.merge(sorted_df_pd_dev[:i])['sentence_id'])\n",
    "    c = list(df_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "#     print(s,c)\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    dev_intersection_p.append(jaccard_similarity(s, c)*100)\n",
    "print(len(dev_intersection_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.merge(sorted_df_pd_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction distance from lca test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_test.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_lca']]\n",
    "df1 = df1.rename(columns={'d_lca':'d_lca_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_test.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_lca']]\n",
    "df2 = df2.rename(columns={'d_lca': 'd_lca_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_test.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_lca']]\n",
    "df3 = df3.rename(columns={'d_lca': 'd_lca_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "dflca = df1.merge(df2).merge(df3)\n",
    "dflca['d_lca'] = (dflca['d_lca_parnn'] + dflca['d_lca_lstm'] + dflca['d_lca_gcn']) / 3\n",
    "dflca = dflca.merge(df_test_len)\n",
    "dflca = dflca.merge(df_test_fk)\n",
    "dflca = dflca.merge(df_test_ef)\n",
    "print('\\n')\n",
    "print(len(dflca))\n",
    "dflca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflca = dflca.sort_values(by=['d_lca'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_l = []\n",
    "for i in range(len(sorted_dflca)):\n",
    "    n_dp = len(df_test.merge(sorted_dflca[:i]))\n",
    "    freq_l.append(n_dp)\n",
    "print(len(freq_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_l = []\n",
    "for i in range(5, len(sorted_dflca)):\n",
    "    s = list(df_test.merge(sorted_dflca[:i])['sentence_id'])\n",
    "    c = list(df_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "#     print(s,c)\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    intersection_l.append(jaccard_similarity(s, c)*100)\n",
    "print(len(intersection_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.merge(sorted_dflca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction distance from lca dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_dev.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_lca']]\n",
    "df1 = df1.rename(columns={'d_lca':'d_lca_parnn'})\n",
    "print(len(df1))\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_dev.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_lca']]\n",
    "df2 = df2.rename(columns={'d_lca': 'd_lca_lstm'})\n",
    "print(len(df2))\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_dev.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_lca']]\n",
    "df3 = df3.rename(columns={'d_lca': 'd_lca_gcn'})\n",
    "print(len(df3))\n",
    "\n",
    "dflca_dev = df1.merge(df2).merge(df3)\n",
    "dflca_dev['d_lca'] = (dflca_dev['d_lca_parnn'] + dflca_dev['d_lca_lstm'] + dflca_dev['d_lca_gcn']) / 3\n",
    "dflca_dev = dflca_dev.merge(df_dev_len)\n",
    "dflca_dev = dflca_dev.merge(df_dev_fk)\n",
    "dflca_dev = dflca_dev.merge(df_dev_ef)\n",
    "print('\\n')\n",
    "print(len(dflca_dev))\n",
    "dflca_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflca_dev = dflca_dev.sort_values(by=['d_lca'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_freq_l = []\n",
    "for i in range(len(sorted_dflca_dev)):\n",
    "    n_dp = len(df_dev.merge(sorted_dflca_dev[:i]))\n",
    "    dev_freq_l.append(n_dp)\n",
    "print(len(dev_freq_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_intersection_l = []\n",
    "for i in range(5,len(sorted_dflca_dev)):\n",
    "    s = list(df_dev.merge(sorted_dflca_dev[:i])['sentence_id'])\n",
    "    c = list(df_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "#     print(jaccard_similarity(s, c)*100)\n",
    "    dev_intersection_l.append(jaccard_similarity(s, c)*100)\n",
    "print(len(dev_intersection_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.merge(sorted_dflca_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.merge(sorted_dflca)['sentence_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.merge(sorted_dflca_dev)['sentence_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of Distance on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('new_parnn_test.tsv', sep='\\t')\n",
    "# df1['d_l/d_p'] = df1['d_lca'] / df1['d_prediction']\n",
    "# df1 = df1.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df1 = df1.rename(columns={'d_l/d_p':'d_l/d_p_parnn'})\n",
    "\n",
    "# df2 = pd.read_csv('new_lstm_test.tsv', sep='\\t')\n",
    "# df2['d_l/d_p'] = df2['d_lca'] / df2['d_prediction']\n",
    "# df2 = df2.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df2 = df2.rename(columns={'d_l/d_p':'d_l/d_p_lstm'})\n",
    "\n",
    "# df3 = pd.read_csv('new_cgcn_test.tsv', sep='\\t')\n",
    "# df3['d_l/d_p'] = df3['d_lca'] / df3['d_prediction']\n",
    "# df3 = df3.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df3 = df3.rename(columns={'d_l/d_p':'d_l/d_p_gcn'})\n",
    "\n",
    "# df = df1.merge(df2).merge(df3)\n",
    "# df['d_l/d_p'] = (df['d_l/d_p_parnn'] + df['d_l/d_p_lstm'] + df['d_l/d_p_gcn']) / 3\n",
    "# df = df.merge(df_test_len)\n",
    "# df = df.merge(df_test_fk)\n",
    "# df = df.merge(df_test_ef)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_dfratio = df.sort_values(by=['d_l/d_p'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_r = []\n",
    "# for i in range(len(sorted_dfratio)):\n",
    "#     n_dp = len(df_test.merge(sorted_dfratio[:i]))\n",
    "#     freq_r.append(n_dp)\n",
    "# print(len(freq_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection_r = []\n",
    "# for i in range(5, len(sorted_dfratio)):\n",
    "#     s = list(df_test.merge(sorted_dfratio[:i])['sentence_id'])\n",
    "#     c = list(df_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "# #     print(s,c)\n",
    "# #     print(jaccard_similarity(s, c)*100)\n",
    "#     intersection_r.append(jaccard_similarity(s, c)*100)\n",
    "# print(len(intersection_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio of distance on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('parnn_dev.tsv', sep='\\t')\n",
    "# df1['d_l/d_p'] = df1['d_lca'] / df1['d_prediction']\n",
    "# df1 = df1.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df1 = df1.rename(columns={'d_l/d_p':'d_l/d_p_parnn'})\n",
    "\n",
    "# df2 = pd.read_csv('lstm_dev.tsv', sep='\\t')\n",
    "# df2['d_l/d_p'] = df2['d_lca'] / df2['d_prediction']\n",
    "# df2 = df2.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df2 = df2.rename(columns={'d_l/d_p':'d_l/d_p_lstm'})\n",
    "\n",
    "# df3 = pd.read_csv('gcn_dev.tsv', sep='\\t')\n",
    "# df3['d_l/d_p'] = df3['d_lca'] / df3['d_prediction']\n",
    "# df3 = df3.loc[:, ['sentence_id', 'd_l/d_p']]\n",
    "# df3 = df3.rename(columns={'d_l/d_p':'d_l/d_p_gcn'})\n",
    "\n",
    "# df = df1.merge(df2).merge(df3)\n",
    "# df['d_l/d_p'] = (df['d_l/d_p_parnn'] + df['d_l/d_p_lstm'] + df['d_l/d_p_gcn']) / 3\n",
    "# df = df.merge(df_dev_len)\n",
    "# df = df.merge(df_dev_fk)\n",
    "# df = df.merge(df_dev_ef)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('d_ratio_dev.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df = df.sort_values(by=['d_l/d_p'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dev.merge(sorted_df[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'Confidence':freq_c, 'GD':freq_p, 'LD':freq_l})\n",
    "dev_freq = pd.DataFrame({'Confidence':dev_freq_c, 'GD':dev_freq_p, 'LD':dev_freq_l})\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=freq)\n",
    "plt.ylabel('% of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "plt.yticks(np.arange(0, 672, 67.1))\n",
    "plt.grid()\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=671))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_freq)\n",
    "# plt.ylabel('Common with TACREV')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "plt.yticks(np.arange(0, 1062, 106.1))\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1061))\n",
    "\n",
    "plt.savefig('common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = pd.DataFrame({'Confidence vs Confidence':intersection_c, 'GD vs Confidence':intersection_p, 'LD vs Confidence':intersection_l})\n",
    "dev_intersection = pd.DataFrame({'Confidence vs Confidence':dev_intersection_c, 'GD vs Confidence':dev_intersection_p, 'LD vs Confidence':dev_intersection_l})\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=intersection)\n",
    "plt.ylabel('Jaccard Similarity', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 643, 64.2))\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_intersection)\n",
    "# plt.ylabel('Common with TACREV')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 1050, 104.9))\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.scatter(range(len(freq)), np.array(freq.confidence), marker='o')\n",
    "# plt.scatter(range(len(freq)), np.array(freq.d_prediction), marker='+')\n",
    "# plt.scatter(range(len(freq)), np.array(freq.d_lca), marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of correct and incorrect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('new_parnn_test.tsv', sep='\\t')\n",
    "df1['correctness'] = 0\n",
    "df1['model'] = 'parnn'\n",
    "df1 = df1.merge(df_test_len)\n",
    "df1 = df1.merge(df_test_fk)\n",
    "df1 = df1.merge(df_test_ef)\n",
    "\n",
    "df1c = pd.read_csv('new_crct_parnn_test.tsv', sep='\\t')\n",
    "df1c['correctness'] = 1\n",
    "df1c['model'] = 'parnn'\n",
    "df1c = df1c.merge(df_test_len)\n",
    "df1c = df1c.merge(df_test_fk)\n",
    "df1c = df1c.merge(df_test_ef)\n",
    "\n",
    "df2 = pd.read_csv('new_lstm_test.tsv', sep='\\t')\n",
    "df2['correctness'] = 0\n",
    "df2['model'] = 'lstm'\n",
    "df2 = df2.merge(df_test_len)\n",
    "df2 = df2.merge(df_test_fk)\n",
    "df2 = df2.merge(df_test_ef)\n",
    "\n",
    "df2c = pd.read_csv('new_crct_lstm_test.tsv', sep='\\t')\n",
    "df2c['correctness'] = 1\n",
    "df2c['model'] = 'lstm'\n",
    "df2c = df2c.merge(df_test_len)\n",
    "df2c = df2c.merge(df_test_fk)\n",
    "df2c = df2c.merge(df_test_ef)\n",
    "\n",
    "df3 = pd.read_csv('new_cgcn_test.tsv', sep='\\t')\n",
    "df3['correctness'] = 0\n",
    "df3['model'] = 'cgcn'\n",
    "df3 = df3.merge(df_test_len)\n",
    "df3 = df3.merge(df_test_fk)\n",
    "df3 = df3.merge(df_test_ef)\n",
    "\n",
    "df3c = pd.read_csv('new_crct_cgcn_test.tsv', sep='\\t')\n",
    "df3c['correctness'] = 1\n",
    "df3c['model'] = 'cgcn'\n",
    "df3c = df3c.merge(df_test_len)\n",
    "df3c = df3c.merge(df_test_fk)\n",
    "df3c = df3c.merge(df_test_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df1[df1.ground_truth != df1.prediction]\n",
    "d = d[(d.ground_truth != 0) & (d.prediction != 0)]\n",
    "d.sort_values(by=['d_lca'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = df1.append(df1c, ignore_index=True)\n",
    "data2 = df2.append(df2c, ignore_index=True)\n",
    "data3 = df3.append(df3c, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1, x='correctness', y='sentence_len')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2, x='correctness', y='sentence_len')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3, x='correctness', y='sentence_len')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = plt.hist([np.array(df1.sentence_len), np.array(df1c.sentence_len)], bins=[10,20,30,40,50,60,70,80,90,100], label=['0', '1'])\n",
    "ax1 = plt.title('PARNN')\n",
    "ax1 = plt.xlabel('Sentence length')\n",
    "ax1 = plt.legend()\n",
    "\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = plt.hist([np.array(df2.sentence_len), np.array(df2c.sentence_len)], bins=[10,20,30,40,50,60,70,80,90,100], label=['0', '1'])\n",
    "ax2 = plt.title('LSTM')\n",
    "ax2 = plt.xlabel('Sentence length')\n",
    "ax2 = plt.legend()\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = plt.hist([np.array(df3.sentence_len), np.array(df3c.sentence_len)], bins=[10,20,30,40,50,60,70,80,90,100], label=['0', '1'])\n",
    "ax3 = plt.title('CGCN')\n",
    "ax3 = plt.xlabel('Sentence length')\n",
    "ax3 = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1, x='correctness', y='f_k_score')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2, x='correctness', y='f_k_score')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3, x='correctness', y='f_k_score')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1, x='correctness', y='entities_distance')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2, x='correctness', y='entities_distance')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3, x='correctness', y='entities_distance')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1, x='correctness', y='entities_count')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2, x='correctness', y='entities_count')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3, x='correctness', y='entities_count')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dev = pd.read_csv('new_parnn_dev.tsv', sep='\\t')\n",
    "df1_dev['correctness'] = 0\n",
    "df1_dev['model'] = 'parnn'\n",
    "df1_dev = df1_dev.merge(df_dev_len)\n",
    "df1_dev = df1_dev.merge(df_dev_fk)\n",
    "df1_dev = df1_dev.merge(df_dev_ef)\n",
    "\n",
    "df1c_dev = pd.read_csv('new_crct_parnn_dev.tsv', sep='\\t')\n",
    "df1c_dev['correctness'] = 1\n",
    "df1c_dev['model'] = 'parnn'\n",
    "df1c_dev = df1c_dev.merge(df_dev_len)\n",
    "df1c_dev = df1c_dev.merge(df_dev_fk)\n",
    "df1c_dev = df1c_dev.merge(df_dev_ef)\n",
    "\n",
    "\n",
    "df2_dev = pd.read_csv('new_lstm_dev.tsv', sep='\\t')\n",
    "df2_dev['correctness'] = 0\n",
    "df2_dev['model'] = 'lstm'\n",
    "df2_dev = df2_dev.merge(df_dev_len)\n",
    "df2_dev = df2_dev.merge(df_dev_fk)\n",
    "df2_dev = df2_dev.merge(df_dev_ef)\n",
    "\n",
    "df2c_dev = pd.read_csv('new_crct_lstm_dev.tsv', sep='\\t')\n",
    "df2c_dev['correctness'] = 1\n",
    "df2c_dev['model'] = 'lstm'\n",
    "df2c_dev = df2c_dev.merge(df_dev_len)\n",
    "df2c_dev = df2c_dev.merge(df_dev_fk)\n",
    "df2c_dev = df2c_dev.merge(df_dev_ef)\n",
    "\n",
    "df3_dev = pd.read_csv('new_cgcn_dev.tsv', sep='\\t')\n",
    "df3_dev['correctness'] = 0\n",
    "df3_dev['model'] = 'cgcn'\n",
    "df3_dev = df3_dev.merge(df_dev_len)\n",
    "df3_dev = df3_dev.merge(df_dev_fk)\n",
    "df3_dev = df3_dev.merge(df_dev_ef)\n",
    "\n",
    "df3c_dev = pd.read_csv('new_crct_cgcn_dev.tsv', sep='\\t')\n",
    "df3c_dev['correctness'] = 1\n",
    "df3c_dev['model'] = 'cgcn'\n",
    "df3c_dev = df3c_dev.merge(df_dev_len)\n",
    "df3c_dev = df3c_dev.merge(df_dev_fk)\n",
    "df3c_dev = df3c_dev.merge(df_dev_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_dev = df1_dev.append(df1c_dev, ignore_index=True)\n",
    "data2_dev = df2_dev.append(df2c_dev, ignore_index=True)\n",
    "data3_dev = df3_dev.append(df3c_dev, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1_dev, x='correctness', y='sentence_len')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2_dev, x='correctness', y='sentence_len')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3_dev, x='correctness', y='sentence_len')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1_dev, x='correctness', y='f_k_score')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2_dev, x='correctness', y='f_k_score')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3_dev, x='correctness', y='f_k_score')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1_dev, x='correctness', y='entities_distance')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2_dev, x='correctness', y='entities_distance')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3_dev, x='correctness', y='entities_distance')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "ax1 = plt.subplot(131)\n",
    "ax1 = sns.violinplot(data=data1_dev, x='correctness', y='entities_count')\n",
    "ax1 = plt.title('PARNN')\n",
    "\n",
    "ax2 = plt.subplot(132)\n",
    "ax2 = sns.violinplot(data=data2_dev, x='correctness', y='entities_count')\n",
    "ax2 = plt.title('LSTM')\n",
    "\n",
    "ax3 = plt.subplot(133)\n",
    "ax3 = sns.violinplot(data=data3_dev, x='correctness', y='entities_count')\n",
    "ax3 = plt.title('CGCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying label change in TACREV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_ID = {'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'per:employee_of': 3, 'org:alternate_names': 4, 'org:country_of_headquarters': 5, 'per:countries_of_residence': 6, 'org:city_of_headquarters': 7, 'per:cities_of_residence': 8, 'per:age': 9, 'per:stateorprovinces_of_residence': 10, 'per:origin': 11, 'org:subsidiaries': 12, 'org:parents': 13, 'per:spouse': 14, 'org:stateorprovince_of_headquarters': 15, 'per:children': 16, 'per:other_family': 17, 'per:alternate_names': 18, 'org:members': 19, 'per:siblings': 20, 'per:schools_attended': 21, 'per:parents': 22, 'per:date_of_death': 23, 'org:member_of': 24, 'org:founded_by': 25, 'org:website': 26, 'per:cause_of_death': 27, 'org:political/religious_affiliation': 28, 'org:founded': 29, 'per:city_of_death': 30, 'org:shareholders': 31, 'org:number_of_employees/members': 32, 'per:date_of_birth': 33, 'per:city_of_birth': 34, 'per:charges': 35, 'per:stateorprovince_of_death': 36, 'per:religion': 37, 'per:stateorprovince_of_birth': 38, 'per:country_of_birth': 39, 'org:dissolved': 40, 'per:country_of_death': 41, 'per:nationality': 42, 'org:location_of_headquarters':43, 'per:location_of_birth':44, 'per:location_of_death':45, 'per:location_of_residence':46, 'per:family':47, 'per-per':48, 'per-org':49, 'per-misc':50, 'per-loc':51, 'org-per':52, 'org-org':53, 'org-misc':54, 'org-loc':55, 'per':56, 'org':57, 'relation':58, 'root':59}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacrev_test = pd.read_json('./tacrev_patch/test_patch.json')\n",
    "tacrev_test['relation'] = [LABEL_TO_ID[rel] for rel in list(tacrev_test['relation'])]\n",
    "tacrev_test = tacrev_test.rename(columns={'id':'sentence_id'})\n",
    "tacrev_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_dict = dict(zip(tacrev_test.sentence_id, tacrev_test.relation))\n",
    "tt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conf = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_dfconf.sentence_id, start=1):\n",
    "    parnn_pred = int(df1[df1['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2[df2['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3[df3['sentence_id'] == sid]['prediction'])\n",
    "    if sid in tt_dict:\n",
    "        reannotation = tt_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_test.merge(sorted_dfconf[:i])), count/len(df_test.merge(sorted_dfconf[:i]))*100)\n",
    "            match_conf.append(count/len(df_test.merge(sorted_dfconf[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dpred = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_df_pd.sentence_id, start=1):\n",
    "    parnn_pred = int(df1[df1['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2[df2['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3[df3['sentence_id'] == sid]['prediction'])\n",
    "    if sid in tt_dict:\n",
    "        reannotation = tt_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_test.merge(sorted_df_pd[:i])), count/len(df_test.merge(sorted_df_pd[:i]))*100)\n",
    "            match_dpred.append(count/len(df_test.merge(sorted_df_pd[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_dpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dlca = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_dflca.sentence_id, start=1):\n",
    "    parnn_pred = int(df1[df1['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2[df2['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3[df3['sentence_id'] == sid]['prediction'])\n",
    "    if sid in tt_dict:\n",
    "        reannotation = tt_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_test.merge(sorted_dflca[:i])), count/len(df_test.merge(sorted_dflca[:i]))*100)\n",
    "            match_dlca.append(count/len(df_test.merge(sorted_dflca[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_dlca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacrev_dev = pd.read_json('./tacrev_patch/dev_patch.json')\n",
    "tacrev_dev['relation'] = [LABEL_TO_ID[rel] for rel in list(tacrev_dev['relation'])]\n",
    "tacrev_dev = tacrev_dev.rename(columns={'id':'sentence_id'})\n",
    "tacrev_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_dict = dict(zip(tacrev_dev.sentence_id, tacrev_dev.relation))\n",
    "td_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conf_dev = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_dfconf_dev.sentence_id, start=1):\n",
    "    parnn_pred = int(df1_dev[df1_dev['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2_dev[df2_dev['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3_dev[df3_dev['sentence_id'] == sid]['prediction'])\n",
    "    if sid in td_dict:\n",
    "        reannotation = td_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_dev.merge(sorted_dfconf_dev[:i])), count/len(df_dev.merge(sorted_dfconf_dev[:i]))*100)\n",
    "            match_conf_dev.append(count/len(df_dev.merge(sorted_dfconf_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_conf_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dpred_dev = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_df_pd_dev.sentence_id, start=1):\n",
    "    parnn_pred = int(df1_dev[df1_dev['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2_dev[df2_dev['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3_dev[df3_dev['sentence_id'] == sid]['prediction'])\n",
    "    if sid in td_dict:\n",
    "        reannotation = td_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_dev.merge(sorted_df_pd_dev[:i])), count/len(df_dev.merge(sorted_df_pd_dev[:i]))*100)\n",
    "            match_dpred_dev.append(count/len(df_dev.merge(sorted_df_pd_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_dpred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dlca_dev = []\n",
    "count = 0\n",
    "for i, sid in enumerate(sorted_dflca_dev.sentence_id, start=1):\n",
    "    parnn_pred = int(df1_dev[df1_dev['sentence_id'] == sid]['prediction'])\n",
    "    lstm_pred = int(df2_dev[df2_dev['sentence_id'] == sid]['prediction'])\n",
    "    cgcn_pred = int(df3_dev[df3_dev['sentence_id'] == sid]['prediction'])\n",
    "    if sid in td_dict:\n",
    "        reannotation = td_dict[sid]\n",
    "        if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == cgcn_pred:\n",
    "            count += 1\n",
    "            print(i, reannotation, count, len(df_dev.merge(sorted_dflca_dev[:i])), count/len(df_dev.merge(sorted_dflca_dev[:i]))*100)\n",
    "            match_dlca_dev.append(count/len(df_dev.merge(sorted_dflca_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(match_dlca_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = pd.DataFrame({'Confidence':match_conf, 'GD':match_dpred, 'LD':match_dlca})\n",
    "dev_match = pd.DataFrame({'Confidence':match_conf_dev, 'GD':match_dpred_dev, 'LD':match_dlca_dev})\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=match)\n",
    "plt.ylabel('Model Agreement', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_match)\n",
    "# plt.ylabel('Prediction of atleast one model matching TACREV')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 based on confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./original_dataset_results_12_2_2021/parnn_train.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'confidence']]\n",
    "df1 = df1.rename(columns={'confidence':'confidence_parnn'})\n",
    "\n",
    "df2 = pd.read_csv('./original_dataset_results_12_2_2021/lstm_train.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'confidence']]\n",
    "df2 = df2.rename(columns={'confidence': 'confidence_lstm'})\n",
    "\n",
    "df3 = pd.read_csv('./original_dataset_results_12_2_2021/gcn_train.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'confidence']]\n",
    "df3 = df3.rename(columns={'confidence': 'confidence_gcn'})\n",
    "\n",
    "dfconf = df1.merge(df2).merge(df3)\n",
    "dfconf['confidence'] = (dfconf['confidence_parnn'] + dfconf['confidence_lstm'] + dfconf['confidence_gcn']) / 3\n",
    "dfconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf = dfconf.sort_values(by=['confidence'], ascending=False)\n",
    "conf_100 = sorted_dfconf[:100]\n",
    "conf_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 based on distance of prediction from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./original_dataset_results_12_2_2021/parnn_train.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_prediction']]\n",
    "df1 = df1.rename(columns={'d_prediction':'d_prediction_parnn'})\n",
    "\n",
    "df2 = pd.read_csv('./original_dataset_results_12_2_2021/lstm_train.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_prediction']]\n",
    "df2 = df2.rename(columns={'d_prediction': 'd_prediction_lstm'})\n",
    "\n",
    "df3 = pd.read_csv('./original_dataset_results_12_2_2021/gcn_train.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_prediction']]\n",
    "df3 = df3.rename(columns={'d_prediction': 'd_prediction_gcn'})\n",
    "\n",
    "dfdpred = df1.merge(df2).merge(df3)\n",
    "dfdpred['d_prediction'] = (dfdpred['d_prediction_parnn'] + dfdpred['d_prediction_lstm'] + dfdpred['d_prediction_gcn']) / 3\n",
    "dfdpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfdpred = dfdpred.sort_values(by=['d_prediction'], ascending=False)\n",
    "dpred_100 = sorted_dfdpred[:100]\n",
    "dpred_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 based on distance of lca from ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./original_dataset_results_12_2_2021/parnn_train.tsv', sep='\\t')\n",
    "df1 = df1.loc[:,['sentence_id', 'd_lca']]\n",
    "df1 = df1.rename(columns={'d_lca':'d_lca_parnn'})\n",
    "\n",
    "df2 = pd.read_csv('./original_dataset_results_12_2_2021/lstm_train.tsv', sep='\\t')\n",
    "df2 = df2.loc[:,['sentence_id', 'd_lca']]\n",
    "df2 = df2.rename(columns={'d_lca': 'd_lca_lstm'})\n",
    "\n",
    "df3 = pd.read_csv('./original_dataset_results_12_2_2021/gcn_train.tsv', sep='\\t')\n",
    "df3 = df3.loc[:,['sentence_id', 'd_lca']]\n",
    "df3 = df3.rename(columns={'d_lca': 'd_lca_gcn'})\n",
    "\n",
    "dfdlca = df1.merge(df2).merge(df3)\n",
    "dfdlca['d_lca'] = (dfdlca['d_lca_parnn'] + dfdlca['d_lca_lstm'] + dfdlca['d_lca_gcn']) / 3\n",
    "dfdlca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfdlca = dfdlca.sort_values(by=['d_lca'], ascending=False)\n",
    "dlca_100 = sorted_dfdlca[:100]\n",
    "dlca_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_list = list(conf_100.sentence_id)\n",
    "dpred_list = list(dpred_100.sentence_id)\n",
    "dlca_list = list(dlca_100.sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_conf = 0\n",
    "\n",
    "for eg in train_data:\n",
    "    print('###',eg['id'])\n",
    "    if eg['id'] in conf_list:\n",
    "        print(eg['id'])\n",
    "        print(' '.join(eg['token']))\n",
    "        print((eg['token'][eg['subj_start']:eg['subj_end']+1], eg['token'][eg['obj_start']:eg['obj_end']+1]))\n",
    "        print(eg['relation'])\n",
    "        correctness = input(\"Is relation correct? (Y/N)\")\n",
    "        if correctness == 'N':\n",
    "            c_conf += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dpred = 0\n",
    "c = 1\n",
    "for eg in train_data:\n",
    "    print('###',eg['id'])\n",
    "    if eg['id'] in dpred_list:\n",
    "        print(c)\n",
    "        c+=1\n",
    "        print(eg['id'])\n",
    "        print(' '.join(eg['token']))\n",
    "        print((eg['token'][eg['subj_start']:eg['subj_end']+1], eg['token'][eg['obj_start']:eg['obj_end']+1]))\n",
    "        print(eg['relation'])\n",
    "        correctness = input(\"Is relation correct? (Y/N)\")\n",
    "        if correctness == 'N':\n",
    "            c_conf += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dlca = 0\n",
    "c = 1\n",
    "for eg in train_data:\n",
    "    print('###',eg['id'])\n",
    "    if eg['id'] in dlca_list:\n",
    "        print(c)\n",
    "        c+=1\n",
    "        print(eg['id'])\n",
    "        print(' '.join(eg['token']))\n",
    "        print((eg['token'][eg['subj_start']:eg['subj_end']+1], eg['token'][eg['obj_start']:eg['obj_end']+1]))\n",
    "        print(eg['relation'])\n",
    "        correctness = input(\"Is relation correct? (Y/N)\")\n",
    "        if correctness == 'N':\n",
    "            c_dlca += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dlca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
