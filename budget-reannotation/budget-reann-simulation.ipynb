{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import textstat as txt\n",
    "from itertools import groupby\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import pickle\n",
    "# from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TACRED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('./../dataset/tacred/json/train.json'))\n",
    "print(\"Number of Training instances :: {}\".format(len(train_data)))\n",
    "\n",
    "\n",
    "dev_data = json.load(open('./../dataset/tacred/json/dev.json'))\n",
    "print(\"Number of Dev instances :: {}\".format(len(dev_data)))\n",
    "\n",
    "test_data = json.load(open('./../dataset/tacred/json/test.json'))\n",
    "print(\"Number of Test instances :: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacred_test = pd.DataFrame({'sentence_id':[eg['id'] for eg in test_data]})\n",
    "tacred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tacred_dev = pd.DataFrame({'sentence_id':[eg['id'] for eg in dev_data]})\n",
    "tacred_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Re-TACRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_test = json.load(open('./../dataset/tacred/json/Re-TACRED/test_id2label.json'))\n",
    "print(\"Total Number of instances in ReTACRED-Reduced test set  :: {}\".format(len(re_test)))\n",
    "count = 0\n",
    "retacred_test = dict()\n",
    "for example in test_data:\n",
    "    sid = example['id']\n",
    "    rel = example['relation'] \n",
    "    if sid in re_test and rel != re_test[sid]:\n",
    "        count += 1\n",
    "#         print(count, sid, rel)\n",
    "        retacred_test[sid] = rel\n",
    "    \n",
    "sentence_id = list(retacred_test.keys())\n",
    "\n",
    "retacred_test = pd.DataFrame({'sentence_id':sentence_id})\n",
    "print(\"Number of incorrectly labeled test instances in TACRED test  :: {}\".format(len(retacred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev = json.load(open('./../dataset/tacred/json/Re-TACRED/dev_id2label.json'))\n",
    "print(\"Total Number of instances in ReTACRED-Reduced dev set  :: {}\".format(len(re_dev)))\n",
    "count = 0\n",
    "retacred_dev = dict()\n",
    "for example in dev_data:\n",
    "    sid = example['id']\n",
    "    rel = example['relation'] \n",
    "    if sid in re_dev and rel != re_dev[sid]:\n",
    "        count += 1\n",
    "#         print(count, sid, rel)\n",
    "        retacred_dev[sid] = rel\n",
    "    \n",
    "sentence_id = list(retacred_dev.keys())\n",
    "\n",
    "retacred_dev = pd.DataFrame({'sentence_id':sentence_id})\n",
    "print(\"Number of incorrectly labeled test instances in TACRED dev  :: {}\".format(len(retacred_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train = json.load(open('./../dataset/tacred/json/Re-TACRED/train_id2label.json'))\n",
    "print(\"Total Number of instances in ReTACRED-Reduced train set  :: {}\".format(len(re_train)))\n",
    "count = 0\n",
    "retacred_train = dict()\n",
    "for example in train_data:\n",
    "    sid = example['id']\n",
    "    rel = example['relation'] \n",
    "    if sid in re_train and rel != re_train[sid]:\n",
    "        count += 1\n",
    "#         print(count, sid, rel)\n",
    "        retacred_train[sid] = rel\n",
    "    \n",
    "sentence_id = list(retacred_train.keys())\n",
    "\n",
    "retacred_train = pd.DataFrame({'sentence_id':sentence_id})\n",
    "print(\"Number of incorrectly labeled test instances in TACRED train  :: {}\".format(len(retacred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTACRED Baseline: Random Picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_re = []\n",
    "for i in range(len(tacred_test)):\n",
    "    n_dp = len(retacred_test.merge(tacred_test[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_freq_re.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_freq_re = []\n",
    "for i in range(len(tacred_dev)):\n",
    "    n_dp = len(retacred_dev.merge(tacred_dev[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_dev_freq_re.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_freq_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2))) / float(len(s1.union(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_inc = pd.read_csv('./tacred/logs/inc_parnn_test.tsv', sep='\\t')\n",
    "parnn_test_c = pd.read_csv('./tacred/logs/c_parnn_test.tsv', sep='\\t')\n",
    "parnn_test = pd.concat([parnn_test_c, parnn_test_inc]).reset_index(drop=True); print(len(parnn_test))\n",
    "\n",
    "lstm_test_inc = pd.read_csv('./tacred/logs/inc_lstm_test.tsv', sep='\\t')\n",
    "lstm_test_c = pd.read_csv('./tacred/logs/c_lstm_test.tsv', sep='\\t')\n",
    "lstm_test = pd.concat([lstm_test_c, lstm_test_inc]).reset_index(drop=True); print(len(lstm_test))\n",
    "\n",
    "bilstm_test_inc = pd.read_csv('./bilstm/logs/inc_bilstm_test.tsv', sep='\\t')\n",
    "bilstm_test_c = pd.read_csv('./bilstm/logs/c_bilstm_test.tsv', sep='\\t')\n",
    "bilstm_test = pd.concat([bilstm_test_c, bilstm_test_inc]).reset_index(drop=True); print(len(bilstm_test))\n",
    "\n",
    "cgcn_test_inc = pd.read_csv('./cgcn/logs/inc_cgcn_test.tsv', sep='\\t')\n",
    "cgcn_test_c = pd.read_csv('./cgcn/logs/c_cgcn_test.tsv', sep='\\t')\n",
    "cgcn_test = pd.concat([cgcn_test_c, cgcn_test_inc]).reset_index(drop=True); print(len(cgcn_test))\n",
    "\n",
    "gcn_test_inc = pd.read_csv('./cgcn/logs/inc_gcn_test.tsv', sep='\\t')\n",
    "gcn_test_c = pd.read_csv('./cgcn/logs/c_gcn_test.tsv', sep='\\t')\n",
    "gcn_test = pd.concat([gcn_test_c, gcn_test_inc]).reset_index(drop=True); print(len(gcn_test))\n",
    "\n",
    "cnn_test_inc = pd.read_csv('./cnn/logs/inc_cnn_test.tsv', sep='\\t')\n",
    "cnn_test_c = pd.read_csv('./cnn/logs/c_cnn_test.tsv', sep='\\t')\n",
    "cnn_test = pd.concat([cnn_test_c, cnn_test_inc]).reset_index(drop=True); print(len(cnn_test))\n",
    "\n",
    "sattn_test_inc = pd.read_csv('./self-attention/logs/inc_self-attn_test.tsv', sep='\\t')\n",
    "sattn_test_c = pd.read_csv('./self-attention/logs/c_self-attn_test.tsv', sep='\\t')\n",
    "sattn_test = pd.concat([sattn_test_c, sattn_test_inc]).reset_index(drop=True); print(len(sattn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_dev_inc = pd.read_csv('./tacred/logs/inc_parnn_dev.tsv', sep='\\t')\n",
    "parnn_dev_c = pd.read_csv('./tacred/logs/c_parnn_dev.tsv', sep='\\t')\n",
    "parnn_dev = pd.concat([parnn_dev_c, parnn_dev_inc]).reset_index(drop=True); print(len(parnn_dev))\n",
    "\n",
    "lstm_dev_inc = pd.read_csv('./tacred/logs/inc_lstm_dev.tsv', sep='\\t')\n",
    "lstm_dev_c = pd.read_csv('./tacred/logs/c_lstm_dev.tsv', sep='\\t')\n",
    "lstm_dev = pd.concat([lstm_dev_c, lstm_dev_inc]).reset_index(drop=True); print(len(lstm_dev))\n",
    "\n",
    "bilstm_dev_inc = pd.read_csv('./bilstm/logs/inc_bilstm_dev.tsv', sep='\\t')\n",
    "bilstm_dev_c = pd.read_csv('./bilstm/logs/c_bilstm_dev.tsv', sep='\\t')\n",
    "bilstm_dev = pd.concat([bilstm_dev_c, bilstm_dev_inc]).reset_index(drop=True); print(len(bilstm_dev))\n",
    "\n",
    "cgcn_dev_inc = pd.read_csv('./cgcn/logs/inc_cgcn_dev.tsv', sep='\\t')\n",
    "cgcn_dev_c = pd.read_csv('./cgcn/logs/c_cgcn_dev.tsv', sep='\\t')\n",
    "cgcn_dev = pd.concat([cgcn_dev_c, cgcn_dev_inc]).reset_index(drop=True); print(len(cgcn_dev))\n",
    "\n",
    "gcn_dev_inc = pd.read_csv('./cgcn/logs/inc_gcn_dev.tsv', sep='\\t')\n",
    "gcn_dev_c = pd.read_csv('./cgcn/logs/c_gcn_dev.tsv', sep='\\t')\n",
    "gcn_dev = pd.concat([gcn_dev_c, gcn_dev_inc]).reset_index(drop=True); print(len(gcn_dev))\n",
    "\n",
    "cnn_dev_inc = pd.read_csv('./cnn/logs/inc_cnn_dev.tsv', sep='\\t')\n",
    "cnn_dev_c = pd.read_csv('./cnn/logs/c_cnn_dev.tsv', sep='\\t')\n",
    "cnn_dev = pd.concat([cnn_dev_c, cnn_dev_inc]).reset_index(drop=True); print(len(cnn_dev))\n",
    "\n",
    "sattn_dev_inc = pd.read_csv('./self-attention/logs/inc_self-attn_dev.tsv', sep='\\t')\n",
    "sattn_dev_c = pd.read_csv('./self-attention/logs/c_self-attn_dev.tsv', sep='\\t')\n",
    "sattn_dev = pd.concat([sattn_dev_c, sattn_dev_inc]).reset_index(drop=True); print(len(sattn_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_train_inc = pd.read_csv('./retacred_results/inc_re-parnn_train.tsv', sep='\\t')\n",
    "parnn_train_c = pd.read_csv('./retacred_results/c_re-parnn_train.tsv', sep='\\t')\n",
    "parnn_train = pd.concat([parnn_train_c, parnn_train_inc]).reset_index(drop=True); print(len(parnn_train))\n",
    "\n",
    "lstm_train_inc = pd.read_csv('./retacred_results/inc_re-lstm_train.tsv', sep='\\t')\n",
    "lstm_train_c = pd.read_csv('./retacred_results/c_re-lstm_train.tsv', sep='\\t')\n",
    "lstm_train = pd.concat([lstm_train_c, lstm_train_inc]).reset_index(drop=True); print(len(lstm_train))\n",
    "\n",
    "bilstm_train_inc = pd.read_csv('./retacred_results/inc_re-bilstm_train.tsv', sep='\\t')\n",
    "bilstm_train_c = pd.read_csv('./retacred_results/c_re-bilstm_train.tsv', sep='\\t')\n",
    "bilstm_train = pd.concat([bilstm_train_c, bilstm_train_inc]).reset_index(drop=True); print(len(bilstm_train))\n",
    "\n",
    "cgcn_train_inc = pd.read_csv('./retacred_results/inc_re-cgcn_train.tsv', sep='\\t')\n",
    "cgcn_train_c = pd.read_csv('./retacred_results/c_re-cgcn_train.tsv', sep='\\t')\n",
    "cgcn_train = pd.concat([cgcn_train_c, cgcn_train_inc]).reset_index(drop=True); print(len(cgcn_train))\n",
    "\n",
    "gcn_train_inc = pd.read_csv('./retacred_results/inc_re-gcn_train.tsv', sep='\\t')\n",
    "gcn_train_c = pd.read_csv('./retacred_results/c_re-gcn_train.tsv', sep='\\t')\n",
    "gcn_train = pd.concat([gcn_train_c, gcn_train_inc]).reset_index(drop=True); print(len(gcn_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_score(row):\n",
    "    if row['ground_truth'] == row['prediction']:\n",
    "        row['confidence'] = -1 * row['confidence']\n",
    "    else:\n",
    "        row['confidence'] = 1 * row['confidence']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'confidence':'confidence_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'confidence':'confidence_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'confidence':'confidence_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'confidence':'confidence_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'confidence':'confidence_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "cnn_test_df = cnn_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "cnn_test_df = cnn_test_df.rename(columns={'confidence':'confidence_cnn'})\n",
    "print(len(cnn_test_df))\n",
    "\n",
    "sattn_test_df = sattn_test.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "sattn_test_df = sattn_test_df.rename(columns={'confidence':'confidence_sattn'})\n",
    "print(len(sattn_test_df))\n",
    "\n",
    "# dfconf = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df).merge(cnn_test_df).merge(sattn_test_df)\n",
    "# dfconf['confidence'] = (dfconf['confidence_parnn'] + dfconf['confidence_lstm'] + dfconf['confidence_bilstm']\n",
    "#                         + dfconf['confidence_cgcn'] + dfconf['confidence_gcn'] + dfconf['confidence_cnn'] + dfconf['confidence_sattn']) / 7\n",
    "\n",
    "dfconf = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dfconf['confidence'] = (dfconf['confidence_parnn'] + dfconf['confidence_lstm'] + dfconf['confidence_bilstm']\n",
    "                        + dfconf['confidence_cgcn'] + dfconf['confidence_gcn']) / 5\n",
    "\n",
    "dfconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf = dfconf.sort_values(by=['confidence'], ascending=False)\n",
    "sorted_dfconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_c = []\n",
    "for i in range(len(sorted_dfconf)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfconf[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_freq_c.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_per_c = []\n",
    "for i in range(1, len(sorted_dfconf)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfconf[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_per_c.append(n_dp)\n",
    "    \n",
    "print(len(re_per_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_intersection_c = []\n",
    "for i in range(5,len(sorted_dfconf)):\n",
    "    s = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_intersection_c.append(js)\n",
    "    \n",
    "print(len(re_intersection_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dfconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between ground truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test.loc[:,['sentence_id', 'dp']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'dp':'dp_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test.loc[:,['sentence_id', 'dp']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'dp':'dp_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test.loc[:,['sentence_id', 'dp']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'dp':'dp_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test.loc[:,['sentence_id', 'dp']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'dp':'dp_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test.loc[:,['sentence_id', 'dp']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'dp':'dp_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "cnn_test_df = cnn_test.loc[:,['sentence_id', 'dp']]\n",
    "cnn_test_df = cnn_test_df.rename(columns={'dp':'dp_cnn'})\n",
    "print(len(cnn_test_df))\n",
    "\n",
    "sattn_test_df = sattn_test.loc[:,['sentence_id', 'dp']]\n",
    "sattn_test_df = sattn_test_df.rename(columns={'dp':'dp_sattn'})\n",
    "print(len(sattn_test_df))\n",
    "\n",
    "# dfpd = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df).merge(cnn_test_df).merge(sattn_test_df)\n",
    "# dfpd['dp'] = (dfpd['dp_parnn'] + dfpd['dp_lstm'] + dfpd['dp_bilstm'] \n",
    "#                         + dfpd['dp_cgcn'] + dfpd['dp_gcn'] + dfpd['dp_cnn'] + dfpd['dp_sattn']) / 7\n",
    "\n",
    "dfpd = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dfpd['dp'] = (dfpd['dp_parnn'] + dfpd['dp_lstm'] + dfpd['dp_bilstm'] \n",
    "                        + dfpd['dp_cgcn'] + dfpd['dp_gcn']) / 5\n",
    "\n",
    "dfpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfpd = dfpd.sort_values(by=['dp'], ascending=False)\n",
    "sorted_dfpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_p = []\n",
    "for i in range(len(sorted_dfpd)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfpd[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_freq_p.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_per_p = []\n",
    "for i in range(1, len(sorted_dfpd)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfpd[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_per_p.append(n_dp)\n",
    "    \n",
    "print(len(re_per_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_intersection_p = []\n",
    "for i in range(5,len(sorted_dfpd)):\n",
    "    s = list(retacred_test.merge(sorted_dfpd[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_intersection_p.append(js)\n",
    "    \n",
    "print(len(re_intersection_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dfpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between ground truth and LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test.loc[:,['sentence_id', 'dl']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'dl':'dl_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test.loc[:,['sentence_id', 'dl']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'dl':'dl_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test.loc[:,['sentence_id', 'dl']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'dl':'dl_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test.loc[:,['sentence_id', 'dl']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'dl':'dl_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test.loc[:,['sentence_id', 'dl']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'dl':'dl_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "cnn_test_df = cnn_test.loc[:,['sentence_id', 'dl']]\n",
    "cnn_test_df = cnn_test_df.rename(columns={'dl':'dl_cnn'})\n",
    "print(len(cnn_test_df))\n",
    "\n",
    "sattn_test_df = sattn_test.loc[:,['sentence_id', 'dl']]\n",
    "sattn_test_df = sattn_test_df.rename(columns={'dl':'dl_sattn'})\n",
    "print(len(sattn_test_df))\n",
    "\n",
    "# dflca = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df).merge(cnn_test_df).merge(sattn_test_df)\n",
    "# dflca['dl'] = (dflca['dl_parnn'] + dflca['dl_lstm'] + dflca['dl_bilstm'] \n",
    "#                   + dflca['dl_cgcn'] + dflca['dl_gcn'] + dflca['dl_cnn'] + dflca['dl_sattn']) / 7\n",
    "\n",
    "dflca = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dflca['dl'] = (dflca['dl_parnn'] + dflca['dl_lstm'] + dflca['dl_bilstm'] \n",
    "                  + dflca['dl_cgcn'] + dflca['dl_gcn']) / 5\n",
    "\n",
    "dflca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflca = dflca.sort_values(by=['dl'], ascending=False)\n",
    "sorted_dflca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_l = []\n",
    "for i in range(len(sorted_dflca)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflca[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_freq_l.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_per_l = []\n",
    "for i in range(1, len(sorted_dflca)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflca[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_per_l.append(n_dp)\n",
    "    \n",
    "print(len(re_per_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_intersection_l = []\n",
    "for i in range(5,len(sorted_dflca)):\n",
    "    s = list(retacred_test.merge(sorted_dflca[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_intersection_l.append(js)\n",
    "    \n",
    "print(len(re_intersection_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dflca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = pd.DataFrame({'TACRev':re_freq_c, 'ReTACRED':re_freq_re, 'GD':re_freq_p, 'LD':re_freq_l})\n",
    "plt.figure(figsize=(10,5))\n",
    "# ax1 = plt.subplot(2,2,1)\n",
    "# ax1 = sns.lineplot(data=freq, dashes=False)\n",
    "# plt.ylabel('Number of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 700, 70))\n",
    "\n",
    "\n",
    "# freq = pd.DataFrame({'TACRev':re_freq_c, 'GD':re_freq_p, 'LD':re_freq_l, 'ReTACRED':re_freq_re})\n",
    "freq = pd.DataFrame({'TACRev':re_freq_c, 'GD':re_freq_p, 'LD':re_freq_l})\n",
    "freq = freq / 1795 * 100\n",
    "ax2 = plt.subplot(1,2,1)\n",
    "ax2 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Percentage of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# freq = pd.DataFrame({'Confidence':re_per_c, 'GD':re_per_p, 'LD':re_per_l, 'RL':re_per_rl})\n",
    "# freq = freq*100\n",
    "# ax3 = plt.subplot(2,2,3)\n",
    "# ax3 = sns.lineplot(data=freq, dashes=False)\n",
    "# plt.ylabel('Ratio of sentences common with LC and Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "intersection = pd.DataFrame({'TACRev vs TACRev':re_intersection_c, 'GD vs TACRev':re_intersection_p,\n",
    "                             'LD vs TACRev':re_intersection_l})\n",
    "ax4 = plt.subplot(1,2,2)\n",
    "ax4 = sns.lineplot(data=intersection, dashes=False)\n",
    "plt.ylabel('Jaccard Similarity', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax4.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# plt.savefig('test-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_dev_df = parnn_dev.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "parnn_dev_df = parnn_dev_df.rename(columns={'confidence':'confidence_parnn'})\n",
    "print(len(parnn_dev_df))\n",
    "\n",
    "lstm_dev_df = lstm_dev.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "lstm_dev_df = lstm_dev_df.rename(columns={'confidence':'confidence_lstm'})\n",
    "print(len(lstm_dev_df))\n",
    "\n",
    "bilstm_dev_df = bilstm_dev.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "bilstm_dev_df = bilstm_dev_df.rename(columns={'confidence':'confidence_bilstm'})\n",
    "print(len(bilstm_dev_df))\n",
    "\n",
    "cgcn_dev_df = cgcn_dev.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "cgcn_dev_df = cgcn_dev_df.rename(columns={'confidence':'confidence_cgcn'})\n",
    "print(len(cgcn_dev_df))\n",
    "\n",
    "gcn_dev_df = gcn_dev.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "gcn_dev_df = gcn_dev_df.rename(columns={'confidence':'confidence_gcn'})\n",
    "print(len(gcn_dev_df))\n",
    "\n",
    "dfconf_dev = parnn_dev_df.merge(lstm_dev_df).merge(bilstm_dev_df).merge(cgcn_dev_df).merge(gcn_dev_df)\n",
    "dfconf_dev['confidence'] = (dfconf_dev['confidence_parnn'] + dfconf_dev['confidence_lstm'] + dfconf_dev['confidence_bilstm'] \n",
    "                            + dfconf_dev['confidence_cgcn'] + dfconf_dev['confidence_gcn']) / 5\n",
    "\n",
    "dfconf_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf_dev = dfconf_dev.sort_values(by=['confidence'], ascending=False)\n",
    "sorted_dfconf_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_freq_c = []\n",
    "for i in range(len(sorted_dfconf_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfconf_dev[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_dev_freq_c.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_freq_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_per_c = []\n",
    "for i in range(1, len(sorted_dfconf_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfconf_dev[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_dev_per_c.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_per_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_intersection_c = []\n",
    "for i in range(5,len(sorted_dfconf_dev)):\n",
    "    s = list(retacred_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    c = list(retacred_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_dev_intersection_c.append(js)\n",
    "    \n",
    "print(len(re_dev_intersection_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_dev.merge(sorted_dfconf_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between ground truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_dev_df = parnn_dev.loc[:,['sentence_id', 'dp']]\n",
    "parnn_dev_df = parnn_dev_df.rename(columns={'dp':'dp_parnn'})\n",
    "print(len(parnn_dev_df))\n",
    "\n",
    "lstm_dev_df = lstm_dev.loc[:,['sentence_id', 'dp']]\n",
    "lstm_dev_df = lstm_dev_df.rename(columns={'dp':'dp_lstm'})\n",
    "print(len(lstm_dev_df))\n",
    "\n",
    "bilstm_dev_df = bilstm_dev.loc[:,['sentence_id', 'dp']]\n",
    "bilstm_dev_df = bilstm_dev_df.rename(columns={'dp':'dp_bilstm'})\n",
    "print(len(bilstm_dev_df))\n",
    "\n",
    "cgcn_dev_df = cgcn_dev.loc[:,['sentence_id', 'dp']]\n",
    "cgcn_dev_df = cgcn_dev_df.rename(columns={'dp':'dp_cgcn'})\n",
    "print(len(cgcn_dev_df))\n",
    "\n",
    "gcn_dev_df = gcn_dev.loc[:,['sentence_id', 'dp']]\n",
    "gcn_dev_df = gcn_dev_df.rename(columns={'dp':'dp_gcn'})\n",
    "print(len(gcn_dev_df))\n",
    "\n",
    "dfpd_dev = parnn_dev_df.merge(lstm_dev_df).merge(bilstm_dev_df).merge(cgcn_dev_df).merge(gcn_dev_df)\n",
    "dfpd_dev['dp'] = (dfpd_dev['dp_parnn'] + dfpd_dev['dp_lstm'] + dfpd_dev['dp_bilstm']\n",
    "                            + dfpd_dev['dp_cgcn'] + dfpd_dev['dp_gcn']) / 5\n",
    "\n",
    "dfpd_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfpd_dev = dfpd_dev.sort_values(by=['dp'], ascending=False)\n",
    "sorted_dfpd_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_freq_p = []\n",
    "for i in range(len(sorted_dfpd_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfpd_dev[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_dev_freq_p.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_freq_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_per_p = []\n",
    "for i in range(1, len(sorted_dfpd_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfpd_dev[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_dev_per_p.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_per_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_intersection_p = []\n",
    "for i in range(5, len(sorted_dfpd_dev)):\n",
    "    s = list(retacred_dev.merge(sorted_dfpd_dev[:i])['sentence_id'])\n",
    "    c = list(retacred_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_dev_intersection_p.append(js)\n",
    "    \n",
    "print(len(re_dev_intersection_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_dev.merge(sorted_dfpd_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between ground truth and lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_dev_df = parnn_dev.loc[:,['sentence_id', 'dl']]\n",
    "parnn_dev_df = parnn_dev_df.rename(columns={'dl':'dl_parnn'})\n",
    "print(len(parnn_dev_df))\n",
    "\n",
    "lstm_dev_df = lstm_dev.loc[:,['sentence_id', 'dl']]\n",
    "lstm_dev_df = lstm_dev_df.rename(columns={'dl':'dl_lstm'})\n",
    "print(len(lstm_dev_df))\n",
    "\n",
    "bilstm_dev_df = bilstm_dev.loc[:,['sentence_id', 'dl']]\n",
    "bilstm_dev_df = bilstm_dev_df.rename(columns={'dl':'dl_bilstm'})\n",
    "print(len(bilstm_dev_df))\n",
    "\n",
    "cgcn_dev_df = cgcn_dev.loc[:,['sentence_id', 'dl']]\n",
    "cgcn_dev_df = cgcn_dev_df.rename(columns={'dl':'dl_cgcn'})\n",
    "print(len(cgcn_dev_df))\n",
    "\n",
    "gcn_dev_df = gcn_dev.loc[:,['sentence_id', 'dl']]\n",
    "gcn_dev_df = gcn_dev_df.rename(columns={'dl':'dl_gcn'})\n",
    "print(len(gcn_dev_df))\n",
    "\n",
    "dflca_dev = parnn_dev_df.merge(lstm_dev_df).merge(bilstm_dev_df).merge(cgcn_dev_df).merge(gcn_dev_df)\n",
    "dflca_dev['dl'] = (dflca_dev['dl_parnn'] + dflca_dev['dl_lstm'] + dflca_dev['dl_bilstm'] \n",
    "                      + dflca_dev['dl_cgcn'] + dflca_dev['dl_gcn']) / 5\n",
    "\n",
    "dflca_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflca_dev = dflca_dev.sort_values(by=['dl'], ascending=False)\n",
    "sorted_dflca_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_freq_l = []\n",
    "for i in range(len(sorted_dflca_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dflca_dev[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_dev_freq_l.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_freq_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_per_l = []\n",
    "for i in range(1, len(sorted_dflca_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dflca_dev[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_dev_per_l.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_per_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_intersection_l = []\n",
    "for i in range(5,len(sorted_dflca_dev)):\n",
    "    s = list(retacred_dev.merge(sorted_dflca_dev[:i])['sentence_id'])\n",
    "    c = list(retacred_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_dev_intersection_l.append(js)\n",
    "    \n",
    "print(len(re_dev_intersection_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_dev.merge(sorted_dflca_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio of distance between ground-truth & LCA and ground-truth and root\n",
    "\n",
    "What percentage of correct path from the root is correctly predicted when started from ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_dev_df = parnn_dev\n",
    "parnn_dev_df['lratio'] = parnn_dev_df.dl / parnn_dev_df.dr\n",
    "parnn_dev_df = parnn_dev_df.loc[:,['sentence_id', 'lratio']]\n",
    "parnn_dev_df = parnn_dev_df.rename(columns={'lratio':'lratio_parnn'})\n",
    "print(len(parnn_dev_df))\n",
    "\n",
    "lstm_dev_df = lstm_dev\n",
    "lstm_dev_df['lratio'] = lstm_dev_df.dl / lstm_dev_df.dr\n",
    "lstm_dev_df = lstm_dev_df.loc[:,['sentence_id', 'lratio']]\n",
    "lstm_dev_df = lstm_dev_df.rename(columns={'lratio':'lratio_lstm'})\n",
    "print(len(lstm_dev_df))\n",
    "\n",
    "bilstm_dev_df = bilstm_dev\n",
    "bilstm_dev_df['lratio'] = bilstm_dev_df.dl / bilstm_dev_df.dr\n",
    "bilstm_dev_df = bilstm_dev_df.loc[:,['sentence_id', 'lratio']]\n",
    "bilstm_dev_df = bilstm_dev_df.rename(columns={'lratio':'lratio_bilstm'})\n",
    "print(len(bilstm_dev_df))\n",
    "\n",
    "cgcn_dev_df = cgcn_dev\n",
    "cgcn_dev_df['lratio'] = cgcn_dev_df.dl / cgcn_dev_df.dr\n",
    "cgcn_dev_df = cgcn_dev_df.loc[:,['sentence_id', 'lratio']]\n",
    "cgcn_dev_df = cgcn_dev_df.rename(columns={'lratio':'lratio_cgcn'})\n",
    "print(len(cgcn_dev_df))\n",
    "\n",
    "gcn_dev_df = gcn_dev\n",
    "gcn_dev_df['lratio'] = gcn_dev_df.dl / gcn_dev_df.dr\n",
    "gcn_dev_df = gcn_dev_df.loc[:,['sentence_id', 'lratio']]\n",
    "gcn_dev_df = gcn_dev_df.rename(columns={'lratio':'lratio_gcn'})\n",
    "print(len(gcn_dev_df))\n",
    "\n",
    "dfrl_dev = parnn_dev_df.merge(lstm_dev_df).merge(bilstm_dev_df).merge(cgcn_dev_df).merge(gcn_dev_df)\n",
    "dfrl_dev['lratio'] = (dfrl_dev['lratio_parnn'] + dfrl_dev['lratio_lstm'] + dfrl_dev['lratio_bilstm'] \n",
    "                  + dfrl_dev['lratio_cgcn'] + dfrl_dev['lratio_gcn']) / 5\n",
    "\n",
    "dfrl_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfrl_dev = dfrl_dev.sort_values(by=['lratio'], ascending=False)\n",
    "sorted_dfrl_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_freq_rl = []\n",
    "for i in range(len(sorted_dfrl_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfrl_dev[:i]))\n",
    "    print(i, n_dp)\n",
    "    re_dev_freq_rl.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_freq_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_per_rl = []\n",
    "for i in range(1, len(sorted_dfrl_dev)):\n",
    "    n_dp = len(retacred_dev.merge(sorted_dfrl_dev[:i])) / i\n",
    "    print(i, n_dp)\n",
    "    re_dev_per_rl.append(n_dp)\n",
    "    \n",
    "print(len(re_dev_per_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_dev_intersection_rl = []\n",
    "for i in range(5,len(sorted_dfrl_dev)):\n",
    "    s = list(retacred_dev.merge(sorted_dfrl_dev[:i])['sentence_id'])\n",
    "    c = list(retacred_dev.merge(sorted_dfconf_dev[:i])['sentence_id'])\n",
    "    js = jaccard_similarity(s, c)*100\n",
    "    print(i, js)\n",
    "    re_dev_intersection_rl.append(js)\n",
    "    \n",
    "print(len(re_dev_intersection_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_dev.merge(sorted_dfrl_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations on Dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = pd.DataFrame({'Confidence':re_dev_freq_c, 'GD':re_dev_freq_p, 'LD':re_dev_freq_l, 'RL':re_dev_freq_rl})\n",
    "plt.figure(figsize=(10,6))\n",
    "# ax1 = plt.subplot(2,2,1)\n",
    "# ax1 = sns.lineplot(data=freq, dashes=False)\n",
    "# plt.ylabel('Number of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 700, 70))\n",
    "\n",
    "\n",
    "freq = pd.DataFrame({'TACRev':re_dev_freq_c, 'GD':re_dev_freq_p, 'LD':re_dev_freq_l, 'ReTACRED':re_dev_freq_re})\n",
    "freq = freq / 5326 * 100\n",
    "ax2 = plt.subplot(1,2,1)\n",
    "ax2 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Percentage of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# freq = pd.DataFrame({'Confidence':re_dev_per_c, 'GD':re_dev_per_p, 'LD':re_dev_per_l, 'RL':re_dev_per_rl})\n",
    "# freq = freq*100\n",
    "# ax3 = plt.subplot(2,2,3)\n",
    "# ax3 = sns.lineplot(data=freq, dashes=False)\n",
    "# plt.ylabel('Ratio of sentences common with LC and Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "# ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "intersection = pd.DataFrame({'TACRev vs TACRev':re_dev_intersection_c, 'GD vs TACRev':re_dev_intersection_p,\n",
    "                             'LD vs TACRev':re_dev_intersection_l})\n",
    "ax4 = plt.subplot(1,2,2)\n",
    "ax4 = sns.lineplot(data=intersection, dashes=False)\n",
    "plt.ylabel('Jaccard Similarity', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "ax4.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('dev-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_train_df = parnn_train.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "parnn_train_df = parnn_train_df.rename(columns={'confidence':'confidence_parnn'})\n",
    "print(len(parnn_train_df))\n",
    "\n",
    "lstm_train_df = lstm_train.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "lstm_train_df = lstm_train_df.rename(columns={'confidence':'confidence_lstm'})\n",
    "print(len(lstm_train_df))\n",
    "\n",
    "bilstm_train_df = bilstm_train.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "bilstm_train_df = bilstm_train_df.rename(columns={'confidence':'confidence_bilstm'})\n",
    "print(len(bilstm_train_df))\n",
    "\n",
    "cgcn_train_df = cgcn_train.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "cgcn_train_df = cgcn_train_df.rename(columns={'confidence':'confidence_cgcn'})\n",
    "print(len(cgcn_train_df))\n",
    "\n",
    "gcn_train_df = gcn_train.apply(change_score, 1).loc[:,['sentence_id', 'confidence']]\n",
    "gcn_train_df = gcn_train_df.rename(columns={'confidence':'confidence_gcn'})\n",
    "print(len(gcn_train_df))\n",
    "\n",
    "dfconf_tr = parnn_train_df.merge(lstm_train_df).merge(bilstm_train_df).merge(cgcn_train_df).merge(gcn_train_df)\n",
    "dfconf_tr['confidence'] = (dfconf_tr['confidence_parnn'] + dfconf_tr['confidence_lstm'] + dfconf_tr['confidence_bilstm']\n",
    "                        + dfconf_tr['confidence_cgcn'] + dfconf_tr['confidence_gcn']) / 5\n",
    "\n",
    "dfconf_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfconf_tr = dfconf_tr.sort_values(by=['confidence'], ascending=False)\n",
    "sorted_dfconf_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tr_freq_c = []\n",
    "for i in range(len(sorted_dfconf_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfconf_tr[:i]))\n",
    "    re_tr_freq_c.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_freq_c))\n",
    "\n",
    "re_tr_per_c = []\n",
    "for i in range(1, len(sorted_dfconf_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfconf_tr[:i])) / i\n",
    "    re_tr_per_c.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_per_c))\n",
    "\n",
    "re_tr_intersection_c = []\n",
    "for i in range(5,len(sorted_dfconf_tr)):\n",
    "    s = list(retacred_train.merge(sorted_dfconf_tr[:i])['sentence_id'])\n",
    "    c = list(retacred_train.merge(sorted_dfconf_tr[:i])['sentence_id'])\n",
    "    re_tr_intersection_c.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_tr_intersection_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_train.merge(sorted_dfconf_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between ground truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_train_df = parnn_train.loc[:,['sentence_id', 'dp']]\n",
    "parnn_train_df = parnn_train_df.rename(columns={'dp':'dp_parnn'})\n",
    "print(len(parnn_train_df))\n",
    "\n",
    "lstm_train_df = lstm_train.loc[:,['sentence_id', 'dp']]\n",
    "lstm_train_df = lstm_train_df.rename(columns={'dp':'dp_lstm'})\n",
    "print(len(lstm_train_df))\n",
    "\n",
    "bilstm_train_df = bilstm_train.loc[:,['sentence_id', 'dp']]\n",
    "bilstm_train_df = bilstm_train_df.rename(columns={'dp':'dp_bilstm'})\n",
    "print(len(bilstm_train_df))\n",
    "\n",
    "cgcn_train_df = cgcn_train.loc[:,['sentence_id', 'dp']]\n",
    "cgcn_train_df = cgcn_train_df.rename(columns={'dp':'dp_cgcn'})\n",
    "print(len(cgcn_train_df))\n",
    "\n",
    "gcn_train_df = gcn_train.loc[:,['sentence_id', 'dp']]\n",
    "gcn_train_df = gcn_train_df.rename(columns={'dp':'dp_gcn'})\n",
    "print(len(gcn_train_df))\n",
    "\n",
    "dfpd_tr = parnn_train_df.merge(lstm_train_df).merge(bilstm_train_df).merge(cgcn_train_df).merge(gcn_train_df)\n",
    "dfpd_tr['dp'] = (dfpd_tr['dp_parnn'] + dfpd_tr['dp_lstm'] + dfpd_tr['dp_bilstm']\n",
    "                        + dfpd_tr['dp_cgcn'] + dfpd_tr['dp_gcn']) / 5\n",
    "\n",
    "dfpd_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfpd_tr = dfpd_tr.sort_values(by=['dp'], ascending=False)\n",
    "sorted_dfpd_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tr_freq_p = []\n",
    "for i in range(len(sorted_dfpd_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfpd_tr[:i]))\n",
    "    re_tr_freq_p.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_freq_p))\n",
    "\n",
    "re_tr_per_p = []\n",
    "for i in range(1, len(sorted_dfpd_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfpd_tr[:i])) / i\n",
    "    re_tr_per_p.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_per_p))\n",
    "\n",
    "re_tr_intersection_p = []\n",
    "for i in range(5,len(sorted_dfpd_tr)):\n",
    "    s = list(retacred_train.merge(sorted_dfpd_tr[:i])['sentence_id'])\n",
    "    c = list(retacred_train.merge(sorted_dfconf_tr[:i])['sentence_id'])\n",
    "    re_tr_intersection_p.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_tr_intersection_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_train.merge(sorted_dfpd_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between ground truth and lca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_train_df = parnn_train.loc[:,['sentence_id', 'dl']]\n",
    "parnn_train_df = parnn_train_df.rename(columns={'dl':'dl_parnn'})\n",
    "print(len(parnn_train_df))\n",
    "\n",
    "lstm_train_df = lstm_train.loc[:,['sentence_id', 'dl']]\n",
    "lstm_train_df = lstm_train_df.rename(columns={'dl':'dl_lstm'})\n",
    "print(len(lstm_train_df))\n",
    "\n",
    "bilstm_train_df = bilstm_train.loc[:,['sentence_id', 'dl']]\n",
    "bilstm_train_df = bilstm_train_df.rename(columns={'dl':'dl_bilstm'})\n",
    "print(len(bilstm_train_df))\n",
    "\n",
    "cgcn_train_df = cgcn_train.loc[:,['sentence_id', 'dl']]\n",
    "cgcn_train_df = cgcn_train_df.rename(columns={'dl':'dl_cgcn'})\n",
    "print(len(cgcn_train_df))\n",
    "\n",
    "gcn_train_df = gcn_train.loc[:,['sentence_id', 'dl']]\n",
    "gcn_train_df = gcn_train_df.rename(columns={'dl':'dl_gcn'})\n",
    "print(len(gcn_train_df))\n",
    "\n",
    "dflca_tr = parnn_train_df.merge(lstm_train_df).merge(bilstm_train_df).merge(cgcn_train_df).merge(gcn_train_df)\n",
    "dflca_tr['dl'] = (dflca_tr['dl_parnn'] + dflca_tr['dl_lstm'] + dflca_tr['dl_bilstm']\n",
    "                        + dflca_tr['dl_cgcn'] + dflca_tr['dl_gcn']) / 5\n",
    "\n",
    "dflca_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflca_tr = dflca_tr.sort_values(by=['dl'], ascending=False)\n",
    "sorted_dflca_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tr_freq_l = []\n",
    "for i in range(len(sorted_dflca_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dflca_tr[:i]))\n",
    "    re_tr_freq_l.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_freq_l))\n",
    "\n",
    "re_tr_per_l = []\n",
    "for i in range(1, len(sorted_dflca_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dflca_tr[:i])) / i\n",
    "    re_tr_per_l.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_per_l))\n",
    "\n",
    "re_tr_intersection_l = []\n",
    "for i in range(5,len(sorted_dflca_tr)):\n",
    "    s = list(retacred_train.merge(sorted_dflca_tr[:i])['sentence_id'])\n",
    "    c = list(retacred_train.merge(sorted_dfconf_tr[:i])['sentence_id'])\n",
    "    re_tr_intersection_l.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_tr_intersection_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_train.merge(sorted_dflca_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio of distance between ground-truth & LCA and ground-truth and root\n",
    "\n",
    "What percentage of correct path from the root is correctly predicted when started from ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_tr_df = parnn_train\n",
    "parnn_tr_df['lratio'] = parnn_tr_df.dl / parnn_tr_df.dr\n",
    "parnn_tr_df = parnn_tr_df.loc[:,['sentence_id', 'lratio']]\n",
    "parnn_tr_df = parnn_tr_df.rename(columns={'lratio':'lratio_parnn'})\n",
    "print(len(parnn_tr_df))\n",
    "\n",
    "lstm_tr_df = lstm_train\n",
    "lstm_tr_df['lratio'] = lstm_tr_df.dl / lstm_tr_df.dr\n",
    "lstm_tr_df = lstm_tr_df.loc[:,['sentence_id', 'lratio']]\n",
    "lstm_tr_df = lstm_tr_df.rename(columns={'lratio':'lratio_lstm'})\n",
    "print(len(lstm_tr_df))\n",
    "\n",
    "bilstm_tr_df = bilstm_train\n",
    "bilstm_tr_df['lratio'] = bilstm_tr_df.dl / bilstm_tr_df.dr\n",
    "bilstm_tr_df = bilstm_tr_df.loc[:,['sentence_id', 'lratio']]\n",
    "bilstm_tr_df = bilstm_tr_df.rename(columns={'lratio':'lratio_bilstm'})\n",
    "print(len(bilstm_tr_df))\n",
    "\n",
    "cgcn_tr_df = cgcn_train\n",
    "cgcn_tr_df['lratio'] = cgcn_tr_df.dl / cgcn_tr_df.dr\n",
    "cgcn_tr_df = cgcn_tr_df.loc[:,['sentence_id', 'lratio']]\n",
    "cgcn_tr_df = cgcn_tr_df.rename(columns={'lratio':'lratio_cgcn'})\n",
    "print(len(cgcn_tr_df))\n",
    "\n",
    "gcn_tr_df = gcn_train\n",
    "gcn_tr_df['lratio'] = gcn_tr_df.dl / gcn_tr_df.dr\n",
    "gcn_tr_df = gcn_tr_df.loc[:,['sentence_id', 'lratio']]\n",
    "gcn_tr_df = gcn_tr_df.rename(columns={'lratio':'lratio_gcn'})\n",
    "print(len(gcn_tr_df))\n",
    "\n",
    "dfrl_tr = parnn_tr_df.merge(lstm_tr_df).merge(bilstm_tr_df).merge(cgcn_tr_df).merge(gcn_tr_df)\n",
    "dfrl_tr['lratio'] = (dfrl_tr['lratio_parnn'] + dfrl_tr['lratio_lstm'] + dfrl_tr['lratio_bilstm'] \n",
    "                  + dfrl_tr['lratio_cgcn'] + dfrl_tr['lratio_gcn']) / 5\n",
    "\n",
    "dfrl_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfrl_tr = dfrl_tr.sort_values(by=['lratio'], ascending=False)\n",
    "sorted_dfrl_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tr_freq_rl = []\n",
    "for i in range(len(sorted_dfrl_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfrl_tr[:i]))\n",
    "    re_tr_freq_rl.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_freq_rl))\n",
    "\n",
    "re_tr_per_rl = []\n",
    "for i in range(1, len(sorted_dfrl_tr)):\n",
    "    n_dp = len(retacred_train.merge(sorted_dfrl_tr[:i])) / i\n",
    "    re_tr_per_rl.append(n_dp)\n",
    "    \n",
    "print(len(re_tr_per_rl))\n",
    "\n",
    "re_tr_intersection_rl = []\n",
    "for i in range(5,len(sorted_dfrl_tr)):\n",
    "    s = list(retacred_train.merge(sorted_dfrl_tr[:i])['sentence_id'])\n",
    "    c = list(retacred_train.merge(sorted_dfconf_tr[:i])['sentence_id'])\n",
    "    re_tr_intersection_rl.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_tr_intersection_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_train.merge(sorted_dfrl_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'Confidence':re_tr_freq_c, 'GD':re_tr_freq_p, 'LD':re_tr_freq_l, 'RL':re_tr_freq_rl})\n",
    "plt.figure(figsize=(20,16))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax1 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Number of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 700, 70))\n",
    "\n",
    "\n",
    "freq = pd.DataFrame({'Confidence':re_tr_freq_c, 'GD':re_tr_freq_p, 'LD':re_tr_freq_l, 'RL':re_tr_freq_rl})\n",
    "freq = freq / 13923 * 100\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax2 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Percentage of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "freq = pd.DataFrame({'Confidence':re_tr_per_c, 'GD':re_tr_per_p, 'LD':re_tr_per_l, 'RL':re_tr_per_rl})\n",
    "freq = freq*100\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "ax3 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Ratio of sentences common with LC and Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "intersection = pd.DataFrame({'Confidence vs Confidence':re_tr_intersection_c, 'GD vs Confidence':re_tr_intersection_p,\n",
    "                             'LD vs Confidence':re_tr_intersection_l, 'RL vs Confidence':re_tr_intersection_rl})\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4 = sns.lineplot(data=intersection, dashes=False)\n",
    "plt.ylabel('Jaccard Similarity', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "ax4.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('train-analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'Confidence':re_freq_c, 'GD':re_freq_p, 'LD':re_freq_l, 'RL':re_freq_rl})\n",
    "dev_freq = pd.DataFrame({'Confidence':re_dev_freq_c, 'GD':re_dev_freq_p, 'LD':re_dev_freq_l, 'RL':re_dev_freq_rl})\n",
    "# tr_freq = pd.DataFrame({'Confidence':re_tr_freq_c, 'GD':re_tr_freq_p, 'LD':re_tr_freq_l, 'RL':re_tr_freq_rl})\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=freq)\n",
    "plt.ylabel('Number of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 700, 70))\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_freq)\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 1100, 110))\n",
    "\n",
    "# ax3 = plt.subplot(1,3,3)\n",
    "# ax3 = sns.lineplot(data=tr_freq)\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('retacred_common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'TACRev':re_freq_c, 'GD':re_freq_p, 'LD':re_freq_l, 'ReTACRED':re_freq_re})\n",
    "freq = freq / 3936 * 100\n",
    "dev_freq = pd.DataFrame({'TACRev':re_dev_freq_c, 'GD':re_dev_freq_p, 'LD':re_dev_freq_l, 'ReTACRED':re_dev_freq_re})\n",
    "dev_freq = dev_freq / 5326 * 100\n",
    "# tr_freq = pd.DataFrame({'Confidence':re_tr_freq_c, 'GD':re_tr_freq_p, 'LD':re_tr_freq_l, 'RL':re_tr_freq_rl})\n",
    "# tr_freq = tr_freq / 13923 * 100\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=freq)\n",
    "plt.ylabel('Percentage of sentences common with Set N', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=12, fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_freq)\n",
    "plt.xlabel('Reannotation Budget', fontsize=12, fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "\n",
    "# ax3 = plt.subplot(1,3,3)\n",
    "# ax3 = sns.lineplot(data=tr_freq)\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "# ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('retacred_common_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = pd.DataFrame({'Confidence':re_per_c, 'GD':re_per_p, 'LD':re_per_l, 'RL':re_per_rl})\n",
    "per = freq\n",
    "dev_per = pd.DataFrame({'Confidence':re_dev_per_c, 'GD':re_dev_per_p, 'LD':re_dev_per_l, 'RL':re_dev_per_rl})\n",
    "dev_per = dev_freq\n",
    "# tr_per = pd.DataFrame({'Confidence':re_tr_per_c, 'GD':re_tr_per_p, 'LD':re_tr_per_l, 'RL':re_tr_per_rl})\n",
    "# tr_per = tr_freq*100\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=per)\n",
    "plt.ylabel('Ratio of sentences common with LC and Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_per)\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# ax3 = plt.subplot(1,3,3)\n",
    "# ax3 = sns.lineplot(data=tr_per)\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "# ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('retacred_percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = pd.DataFrame({'TACRev vs TACRev':re_intersection_c, 'GD vs TACRev':re_intersection_p,\n",
    "                             'LD vs TACRev':re_intersection_l})\n",
    "dev_intersection = pd.DataFrame({'TACRev vs TACRev':re_dev_intersection_c, 'GD vs TACRev':re_dev_intersection_p,\n",
    "                             'LD vs TACRev':re_dev_intersection_l})\n",
    "\n",
    "# tr_intersection = pd.DataFrame({'Confidence vs Confidence':re_tr_intersection_c, 'GD vs Confidence':re_tr_intersection_p,\n",
    "#                              'LD vs Confidence':re_tr_intersection_l, 'RL vs Confidence':re_tr_intersection_rl})\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=intersection)\n",
    "plt.ylabel('Jaccard Similarity', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_intersection)\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# ax3 = plt.subplot(1,3,3)\n",
    "# ax3 = sns.lineplot(data=tr_intersection, dashes=False)\n",
    "# plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "# plt.title('Train Dataset', fontsize='large', fontweight='bold')\n",
    "# ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('retacred_intersection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying label change in TACRED (model predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_ID = {'no_relation': 0, 'org:members': 1, 'per:siblings': 2, 'per:spouse': 3, 'org:country_of_branch': 4, 'per:country_of_death': 5, 'per:parents': 6, 'per:stateorprovinces_of_residence': 7, 'org:top_members/employees': 8, 'org:dissolved': 9, 'org:number_of_employees/members': 10, 'per:stateorprovince_of_death': 11, 'per:origin': 12, 'per:children': 13, 'org:political/religious_affiliation': 14, 'per:city_of_birth': 15, 'per:title': 16, 'org:shareholders': 17, 'per:employee_of': 18, 'org:member_of': 19, 'org:founded_by': 20, 'per:countries_of_residence': 21, 'per:other_family': 22, 'per:religion': 23, 'per:identity': 24, 'per:date_of_birth': 25, 'org:city_of_branch': 26, 'org:alternate_names': 27, 'org:website': 28, 'per:cause_of_death': 29, 'org:stateorprovince_of_branch': 30, 'per:schools_attended': 31, 'per:country_of_birth': 32, 'per:date_of_death': 33, 'per:city_of_death': 34, 'org:founded': 35, 'per:cities_of_residence': 36, 'per:age': 37, 'per:charges': 38, 'per:stateorprovince_of_birth': 39}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading ReTACRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retest_dct = json.load(open('./retacred_patch/test_id2label.json'))\n",
    "retest_dct = {key:LABEL_TO_ID[val] for key, val in retest_dct.items() if key in list(retacred_test.sentence_id)}\n",
    "retacred_test = pd.DataFrame({'sentence_id':list(retest_dct.keys()), 'relation':list(retest_dct.values())})\n",
    "\n",
    "retacred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retacred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redev_dct = json.load(open('./retacred_patch/dev_id2label.json'))\n",
    "redev_dct = {key:LABEL_TO_ID[val] for key, val in redev_dct.items() if key in list(retacred_dev.sentence_id)}\n",
    "retacred_dev = pd.DataFrame({'sentence_id':list(redev_dct.keys()), 'relation':list(redev_dct.values())})\n",
    "\n",
    "retacred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retacred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_dct = json.load(open('./retacred_patch/train_id2label.json'))\n",
    "retrain_dct = {key:LABEL_TO_ID[val] for key, val in retrain_dct.items() if key in list(retacred_train.sentence_id)}\n",
    "retacred_train = pd.DataFrame({'sentence_id':list(retrain_dct.keys()), 'relation':list(retrain_dct.values())})\n",
    "\n",
    "retacred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retacred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conf = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfconf.sentence_id, start=1):\n",
    "    temp_df = retacred_test.merge(sorted_dfconf[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = retest_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_conf.append(count)\n",
    "    prev_index = len(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dpred = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfpd.sentence_id, start=1):\n",
    "    temp_df = retacred_test.merge(sorted_dfpd[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = retest_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_dpred.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "# match_dpred = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dfpd.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in retest_dct:\n",
    "#         reannotation = retest_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_dpred.append(count)\n",
    "#             match_dpred.append(count/len(retacred_test.merge(sorted_dfpd[:i]))*100)\n",
    "# #             print(count/len(retacred_test.merge(sorted_dfpd[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_dpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dlca = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dflca.sentence_id, start=1):\n",
    "    temp_df = retacred_test.merge(sorted_dflca[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = retest_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_dlca.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "\n",
    "# match_dlca = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dflca.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in retest_dct:\n",
    "#         reannotation = retest_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_dlca.append(count)\n",
    "#             match_dlca.append(count/len(retacred_test.merge(sorted_dflca[:i]))*100)\n",
    "# #             print(count/len(retacred_test.merge(sorted_dflca[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_dlca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_rl = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfrl.sentence_id, start=1):\n",
    "    temp_df = retacred_test.merge(sorted_dfrl[:i])\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = retest_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_rl.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "\n",
    "# match_rl = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dfrl.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_test[parnn_test['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_test[lstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_test[bilstm_test['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_test[cgcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_test[gcn_test['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in retest_dct:\n",
    "#         reannotation = retest_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_rl.append(count)\n",
    "#             match_rl.append(count/len(retacred_test.merge(sorted_dfrl[:i]))*100)\n",
    "# #             print(count/len(retacred_test.merge(sorted_dfrl[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Dev Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_conf_dev = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfconf_dev.sentence_id, start=1):\n",
    "    temp_df = retacred_dev.merge(sorted_dfconf_dev[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = redev_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_conf_dev.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "\n",
    "# match_conf_dev = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dfconf_dev.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in redev_dct:\n",
    "#         reannotation = redev_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_conf_dev.append(count)\n",
    "#             match_conf_dev.append(count/len(retacred_dev.merge(sorted_dfconf_dev[:i]))*100)\n",
    "# #             print(count/len(retacred_dev.merge(sorted_dfconf_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_conf_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dpred_dev = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfpd_dev.sentence_id, start=1):\n",
    "    temp_df = retacred_dev.merge(sorted_dfpd_dev[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = redev_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_dpred_dev.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "    \n",
    "\n",
    "# match_dpred_dev = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dfpd_dev.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in redev_dct:\n",
    "#         reannotation = redev_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_dpred_dev.append(count)\n",
    "#             match_dpred_dev.append(count/len(retacred_dev.merge(sorted_dfpd_dev[:i]))*100)\n",
    "# #             print(count/len(retacred_dev.merge(sorted_dfpd_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_dpred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dlca_dev = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dflca_dev.sentence_id, start=1):\n",
    "    temp_df = retacred_dev.merge(sorted_dflca_dev[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = redev_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_dlca_dev.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "# match_dlca_dev = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dflca_dev.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in redev_dct:\n",
    "#         reannotation = redev_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_dlca_dev.append(count)\n",
    "#             match_dlca_dev.append(count/len(retacred_dev.merge(sorted_dflca_dev[:i]))*100)\n",
    "# #             print(count/len(retacred_dev.merge(sorted_dflca_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_dlca_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_rl_dev = []\n",
    "count = 0\n",
    "prev_index = 0\n",
    "for i, _ in enumerate(sorted_dfrl_dev.sentence_id, start=1):\n",
    "    temp_df = retacred_dev.merge(sorted_dfrl_dev[:i])\n",
    "    print(i, prev_index, len(temp_df))\n",
    "    for j, sid in enumerate(temp_df[prev_index:].sentence_id):\n",
    "        parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "        lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "        cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "        reannotation = redev_dct[sid]\n",
    "        if (reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or \n",
    "            reannotation == cgcn_pred or reannotation == gcn_pred):\n",
    "            count+=1\n",
    "    print(i, count)\n",
    "    match_rl_dev.append(count)\n",
    "    prev_index = len(temp_df)\n",
    "\n",
    "# match_rl_dev = []\n",
    "# count = 0\n",
    "# for i, sid in enumerate(sorted_dfrl_dev.sentence_id, start=1):\n",
    "#     parnn_pred = int(parnn_dev[parnn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     lstm_pred = int(lstm_dev[lstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     bilstm_pred = int(bilstm_dev[bilstm_dev['sentence_id'] == sid]['prediction'])\n",
    "#     cgcn_pred = int(cgcn_dev[cgcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     gcn_pred = int(gcn_dev[gcn_dev['sentence_id'] == sid]['prediction'])\n",
    "#     if sid in redev_dct:\n",
    "#         reannotation = redev_dct[sid]\n",
    "#         if reannotation == parnn_pred or reannotation == lstm_pred or reannotation == bilstm_pred or reannotation == cgcn_pred or reannotation == gcn_pred:\n",
    "#             count += 1\n",
    "# #             match_rl_dev.append(count)\n",
    "#             match_rl_dev.append(count/len(retacred_dev.merge(sorted_dfrl_dev[:i]))*100)\n",
    "# #             print(count/len(retacred_dev.merge(sorted_dfrl_dev[:i]))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(match_rl_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = pd.DataFrame({'Confidence':match_conf, 'GD':match_dpred, 'LD':match_dlca, 'RL':match_rl})\n",
    "dev_match = pd.DataFrame({'Confidence':match_conf_dev, 'GD':match_dpred_dev, 'LD':match_dlca_dev, 'RL':match_rl_dev})\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "ax1 = sns.lineplot(data=match, dashes=False)\n",
    "plt.ylabel('Model Agreement', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# ax1.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2 = sns.lineplot(data=dev_match, dashes=False)\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Dev Dataset', fontsize='large', fontweight='bold')\n",
    "# ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "plt.savefig('retacred_match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance analysis after Re-Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(labels, predictions):\n",
    "    num_predicted_labels = 0\n",
    "    num_gold_labels = 0\n",
    "    num_correct_labels = 0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if prediction != 0:\n",
    "            num_predicted_labels += 1\n",
    "        if label != 0:\n",
    "            num_gold_labels += 1\n",
    "            if prediction == label:\n",
    "                num_correct_labels += 1\n",
    "\n",
    "    if num_predicted_labels > 0:\n",
    "        precision = num_correct_labels / num_predicted_labels\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    recall = num_correct_labels / num_gold_labels\n",
    "    if recall == 0.0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = round((2 * precision * recall / (precision + recall)) * 100, 2)\n",
    "        \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_labels = dict(list(zip(list(parnn_test.sentence_id), list(parnn_test.ground_truth))))\n",
    "parnn_preds = dict(list(zip(list(parnn_test.sentence_id), list(parnn_test.prediction))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(tacred_test.sentence_id)\n",
    "labels = [parnn_labels[sid] for sid in sids]\n",
    "predictions = [parnn_preds[sid] for sid in sids]\n",
    "pf1_re = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    pf1_re.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfconf.sentence_id)\n",
    "labels = [parnn_labels[sid] for sid in sids]\n",
    "predictions = [parnn_preds[sid] for sid in sids]\n",
    "pf1_conf = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    pf1_conf.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground_truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfpd.sentence_id)\n",
    "labels = [parnn_labels[sid] for sid in sids]\n",
    "predictions = [parnn_preds[sid] for sid in sids]\n",
    "pf1_pd = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    pf1_pd.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground_truth and LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dflca.sentence_id)\n",
    "labels = [parnn_labels[sid] for sid in sids]\n",
    "predictions = [parnn_preds[sid] for sid in sids]\n",
    "pf1_lca = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    pf1_lca.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfrl.sentence_id)\n",
    "labels = [parnn_labels[sid] for sid in sids]\n",
    "predictions = [parnn_preds[sid] for sid in sids]\n",
    "pf1_ratio = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    pf1_ratio.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'TACREV':pf1_conf, 'ReTACRED':pf1_re, 'GD':pf1_pd, 'LD':pf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('PARNN', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('parnn-f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgcn_labels = dict(list(zip(list(cgcn_test.sentence_id), list(cgcn_test.ground_truth))))\n",
    "cgcn_preds = dict(list(zip(list(cgcn_test.sentence_id), list(cgcn_test.prediction))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(tacred_test.sentence_id)\n",
    "labels = [cgcn_labels[sid] for sid in sids]\n",
    "predictions = [cgcn_preds[sid] for sid in sids]\n",
    "cf1_re = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    cf1_re.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfconf.sentence_id)\n",
    "labels = [cgcn_labels[sid] for sid in sids]\n",
    "predictions = [cgcn_preds[sid] for sid in sids]\n",
    "cf1_conf = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    cf1_conf.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground_truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfpd.sentence_id)\n",
    "labels = [cgcn_labels[sid] for sid in sids]\n",
    "predictions = [cgcn_preds[sid] for sid in sids]\n",
    "cf1_pd = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    cf1_pd.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground truth and LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dflca.sentence_id)\n",
    "labels = [cgcn_labels[sid] for sid in sids]\n",
    "predictions = [cgcn_preds[sid] for sid in sids]\n",
    "cf1_lca = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    cf1_lca.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfrl.sentence_id)\n",
    "labels = [cgcn_labels[sid] for sid in sids]\n",
    "predictions = [cgcn_preds[sid] for sid in sids]\n",
    "cf1_ratio = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    cf1_ratio.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'TACRev':cf1_conf, 'ReTACRED':cf1_re, 'GD':cf1_pd, 'LD':cf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('CGCN', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('cgcn-f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_labels = dict(list(zip(list(gcn_test.sentence_id), list(gcn_test.ground_truth))))\n",
    "gcn_preds = dict(list(zip(list(gcn_test.sentence_id), list(gcn_test.prediction))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(tacred_test.sentence_id)\n",
    "labels = [gcn_labels[sid] for sid in sids]\n",
    "predictions = [gcn_preds[sid] for sid in sids]\n",
    "gf1_re = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    gf1_re.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfconf.sentence_id)\n",
    "labels = [gcn_labels[sid] for sid in sids]\n",
    "predictions = [gcn_preds[sid] for sid in sids]\n",
    "gf1_conf = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    gf1_conf.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground truth and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfpd.sentence_id)\n",
    "labels = [gcn_labels[sid] for sid in sids]\n",
    "predictions = [gcn_preds[sid] for sid in sids]\n",
    "gf1_pd = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    gf1_pd.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance between ground truth and LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dflca.sentence_id)\n",
    "labels = [gcn_labels[sid] for sid in sids]\n",
    "predictions = [gcn_preds[sid] for sid in sids]\n",
    "gf1_lca = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    gf1_lca.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfrl.sentence_id)\n",
    "labels = [gcn_labels[sid] for sid in sids]\n",
    "predictions = [gcn_preds[sid] for sid in sids]\n",
    "gf1_ratio = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    gf1_ratio.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'TACRev':gf1_conf, 'ReTACRED':gf1_re, 'GD':gf1_pd, 'LD':gf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('GCN', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('gcn-f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_labels = dict(list(zip(list(lstm_test.sentence_id), list(lstm_test.ground_truth))))\n",
    "lstm_preds = dict(list(zip(list(lstm_test.sentence_id), list(lstm_test.prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(tacred_test.sentence_id)\n",
    "labels = [lstm_labels[sid] for sid in sids]\n",
    "predictions = [lstm_preds[sid] for sid in sids]\n",
    "lf1_re = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    lf1_re.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfconf.sentence_id)\n",
    "labels = [lstm_labels[sid] for sid in sids]\n",
    "predictions = [lstm_preds[sid] for sid in sids]\n",
    "lf1_conf = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    lf1_conf.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfpd.sentence_id)\n",
    "labels = [lstm_labels[sid] for sid in sids]\n",
    "predictions = [lstm_preds[sid] for sid in sids]\n",
    "lf1_pd = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    lf1_pd.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dflca.sentence_id)\n",
    "labels = [lstm_labels[sid] for sid in sids]\n",
    "predictions = [lstm_preds[sid] for sid in sids]\n",
    "lf1_lca = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    lf1_lca.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfrl.sentence_id)\n",
    "labels = [lstm_labels[sid] for sid in sids]\n",
    "predictions = [lstm_preds[sid] for sid in sids]\n",
    "lf1_ratio = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    lf1_ratio.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'TACRev':lf1_conf, 'ReTACRED':lf1_re, 'GD':lf1_pd, 'LD':lf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('LSTM', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('lstm-f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_labels = dict(list(zip(list(bilstm_test.sentence_id), list(bilstm_test.ground_truth))))\n",
    "bilstm_preds = dict(list(zip(list(bilstm_test.sentence_id), list(bilstm_test.prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(tacred_test.sentence_id)\n",
    "labels = [bilstm_labels[sid] for sid in sids]\n",
    "predictions = [bilstm_preds[sid] for sid in sids]\n",
    "bf1_re = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    bf1_re.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfconf.sentence_id)\n",
    "labels = [bilstm_labels[sid] for sid in sids]\n",
    "predictions = [bilstm_preds[sid] for sid in sids]\n",
    "bf1_conf = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    bf1_conf.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfpd.sentence_id)\n",
    "labels = [bilstm_labels[sid] for sid in sids]\n",
    "predictions = [bilstm_preds[sid] for sid in sids]\n",
    "bf1_pd = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    bf1_pd.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dflca.sentence_id)\n",
    "labels = [bilstm_labels[sid] for sid in sids]\n",
    "predictions = [bilstm_preds[sid] for sid in sids]\n",
    "bf1_lca = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    bf1_lca.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = list(sorted_dfrl.sentence_id)\n",
    "labels = [bilstm_labels[sid] for sid in sids]\n",
    "predictions = [bilstm_preds[sid] for sid in sids]\n",
    "bf1_ratio = []\n",
    "prev = 0\n",
    "for i, sid in enumerate(sids, start=1):\n",
    "    for j in range(i):\n",
    "        if sids[j] in re_test:\n",
    "            labels[j] = LABEL_TO_ID[re_test[sids[j]]]\n",
    "    f1 = calculate_f1(labels, predictions)\n",
    "    bf1_ratio.append(f1)\n",
    "    print(i, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'TACRev':bf1_conf, 'ReTACRED':bf1_re, 'GD':bf1_pd, 'LD':bf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('BiLSTM', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('bilstm-f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'PARNN':pf1_conf, 'CGCN':cf1_conf, 'GCN':gf1_conf, 'LSTM':lf1_conf, 'BiLSTM':bf1_conf})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('TACRev', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('f1-tacrev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'PARNN':pf1_re, 'CGCN':cf1_re, 'GCN':gf1_re, 'LSTM':lf1_re, 'BiLSTM':bf1_re})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('ReTACRED', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('f1-retacred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'PARNN':pf1_pd, 'CGCN':cf1_pd, 'GCN':gf1_pd, 'LSTM':lf1_pd, 'BiLSTM':bf1_pd})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Graph Distance', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('f1-pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = pd.DataFrame({'PARNN':pf1_lca, 'CGCN':cf1_lca, 'GCN':gf1_lca, 'LSTM':lf1_lca, 'BiLSTM':bf1_lca})\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('LCA Distance', fontsize='large', fontweight='bold')\n",
    "\n",
    "plt.savefig('f1-lca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "f1s = pd.DataFrame({'PARNN':pf1_conf, 'CGCN':cf1_conf, 'GCN':gf1_conf, 'LSTM':lf1_conf, 'BiLSTM':bf1_conf})\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax1 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('TACRev', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "f1s = pd.DataFrame({'PARNN':pf1_re, 'CGCN':cf1_re, 'GCN':gf1_re, 'LSTM':lf1_re, 'BiLSTM':bf1_re})\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax2 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('ReTACRED', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "f1s = pd.DataFrame({'PARNN':pf1_pd, 'CGCN':cf1_pd, 'GCN':gf1_pd, 'LSTM':lf1_pd, 'BiLSTM':bf1_pd})\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "ax3 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('Graph Distance', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "f1s = pd.DataFrame({'PARNN':pf1_lca, 'CGCN':cf1_lca, 'GCN':gf1_lca, 'LSTM':lf1_lca, 'BiLSTM':bf1_lca})\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4 = sns.lineplot(data=f1s, dashes=False)\n",
    "plt.ylabel('F1 Score', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize=16, fontweight='bold')\n",
    "plt.title('LCA Distance', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.savefig('f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confidence x lratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test\n",
    "parnn_test_df['lrc'] = (parnn_test_df.dl / parnn_test_df.dr) * parnn_test_df.confidence\n",
    "parnn_test_df = parnn_test_df.loc[:,['sentence_id', 'lrc']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'lrc':'lrc_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test\n",
    "lstm_test_df['lrc'] = (lstm_test_df.dl / lstm_test_df.dr) * lstm_test_df.confidence\n",
    "lstm_test_df = lstm_test_df.loc[:,['sentence_id', 'lrc']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'lrc':'lrc_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test\n",
    "bilstm_test_df['lrc'] = (bilstm_test_df.dl / bilstm_test_df.dr) * bilstm_test_df.confidence\n",
    "bilstm_test_df = bilstm_test_df.loc[:,['sentence_id', 'lrc']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'lrc':'lrc_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test\n",
    "cgcn_test_df['lrc'] = (cgcn_test_df.dl / cgcn_test_df.dr) * cgcn_test_df.confidence\n",
    "cgcn_test_df = cgcn_test_df.loc[:,['sentence_id', 'lrc']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'lrc':'lrc_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test\n",
    "gcn_test_df['lrc'] = (gcn_test_df.dl / gcn_test_df.dr) * gcn_test_df.confidence\n",
    "gcn_test_df = gcn_test_df.loc[:,['sentence_id', 'lrc']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'lrc':'lrc_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "dflrc = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dflrc['lrc'] = (dflrc['lrc_parnn'] + dflrc['lrc_lstm'] + dflrc['lrc_bilstm'] \n",
    "                  + dflrc['lrc_cgcn'] + dflrc['lrc_gcn']) / 5\n",
    "\n",
    "dflrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrc = dflrc.sort_values(by=['lrc'], ascending=False)\n",
    "sorted_dflrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_lrc = []\n",
    "for i in range(len(sorted_dflrc)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrc[:i]))\n",
    "    re_freq_lrc.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_lrc))\n",
    "\n",
    "re_per_lrc = []\n",
    "for i in range(1, len(sorted_dflrc)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrc[:i])) / i\n",
    "    re_per_lrc.append(n_dp)\n",
    "    \n",
    "print(len(re_per_lrc))\n",
    "\n",
    "re_intersection_lrc = []\n",
    "for i in range(5,len(sorted_dflrc)):\n",
    "    s = list(retacred_test.merge(sorted_dflrc[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    re_intersection_lrc.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_intersection_lrc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dflrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### square(confidence) x lratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test\n",
    "parnn_test_df['lrcs'] = (parnn_test_df.dl / parnn_test_df.dr) * np.power(parnn_test_df.confidence, 2)\n",
    "parnn_test_df = parnn_test_df.loc[:,['sentence_id', 'lrcs']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'lrcs':'lrcs_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test\n",
    "lstm_test_df['lrcs'] = (lstm_test_df.dl / lstm_test_df.dr) * np.power(lstm_test_df.confidence, 2)\n",
    "lstm_test_df = lstm_test_df.loc[:,['sentence_id', 'lrcs']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'lrcs':'lrcs_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test\n",
    "bilstm_test_df['lrcs'] = (bilstm_test_df.dl / bilstm_test_df.dr) * np.power(bilstm_test_df.confidence, 2)\n",
    "bilstm_test_df = bilstm_test_df.loc[:,['sentence_id', 'lrcs']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'lrcs':'lrcs_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test\n",
    "cgcn_test_df['lrcs'] = (cgcn_test_df.dl / cgcn_test_df.dr) * np.power(cgcn_test_df.confidence, 2)\n",
    "cgcn_test_df = cgcn_test_df.loc[:,['sentence_id', 'lrcs']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'lrcs':'lrcs_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test\n",
    "gcn_test_df['lrcs'] = (gcn_test_df.dl / gcn_test_df.dr) * np.power(gcn_test_df.confidence, 2)\n",
    "gcn_test_df = gcn_test_df.loc[:,['sentence_id', 'lrcs']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'lrcs':'lrcs_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "dflrcs = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dflrcs['lrcs'] = (dflrcs['lrcs_parnn'] + dflrcs['lrcs_lstm'] + dflrcs['lrcs_bilstm'] \n",
    "                  + dflrcs['lrcs_cgcn'] + dflrcs['lrcs_gcn']) / 5\n",
    "\n",
    "dflrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrcs = dflrcs.sort_values(by=['lrcs'], ascending=False)\n",
    "sorted_dflrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_lrcs = []\n",
    "for i in range(len(sorted_dflrcs)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrcs[:i]))\n",
    "    re_freq_lrcs.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_lrcs))\n",
    "\n",
    "re_per_lrcs = []\n",
    "for i in range(1, len(sorted_dflrcs)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrcs[:i])) / i\n",
    "    re_per_lrcs.append(n_dp)\n",
    "    \n",
    "print(len(re_per_lrcs))\n",
    "\n",
    "re_intersection_lrcs = []\n",
    "for i in range(5,len(sorted_dflrcs)):\n",
    "    s = list(retacred_test.merge(sorted_dflrcs[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    re_intersection_lrcs.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_intersection_lrcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dflrcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqrt(confidence) x lratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test\n",
    "parnn_test_df['lrcsq'] = (parnn_test_df.dl / parnn_test_df.dr) * np.sqrt(parnn_test_df.confidence)\n",
    "parnn_test_df = parnn_test_df.loc[:,['sentence_id', 'lrcsq']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'lrcsq':'lrcsq_parnn'})\n",
    "print(len(parnn_test_df))\n",
    "\n",
    "lstm_test_df = lstm_test\n",
    "lstm_test_df['lrcsq'] = (lstm_test_df.dl / lstm_test_df.dr) * np.sqrt(lstm_test_df.confidence)\n",
    "lstm_test_df = lstm_test_df.loc[:,['sentence_id', 'lrcsq']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'lrcsq':'lrcsq_lstm'})\n",
    "print(len(lstm_test_df))\n",
    "\n",
    "bilstm_test_df = bilstm_test\n",
    "bilstm_test_df['lrcsq'] = (bilstm_test_df.dl / bilstm_test_df.dr) * np.sqrt(bilstm_test_df.confidence)\n",
    "bilstm_test_df = bilstm_test_df.loc[:,['sentence_id', 'lrcsq']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'lrcsq':'lrcsq_bilstm'})\n",
    "print(len(bilstm_test_df))\n",
    "\n",
    "cgcn_test_df = cgcn_test\n",
    "cgcn_test_df['lrcsq'] = (cgcn_test_df.dl / cgcn_test_df.dr) * np.sqrt(cgcn_test_df.confidence)\n",
    "cgcn_test_df = cgcn_test_df.loc[:,['sentence_id', 'lrcsq']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'lrcsq':'lrcsq_cgcn'})\n",
    "print(len(cgcn_test_df))\n",
    "\n",
    "gcn_test_df = gcn_test\n",
    "gcn_test_df['lrcsq'] = (gcn_test_df.dl / gcn_test_df.dr) * np.sqrt(gcn_test_df.confidence)\n",
    "gcn_test_df = gcn_test_df.loc[:,['sentence_id', 'lrcsq']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'lrcsq':'lrcsq_gcn'})\n",
    "print(len(gcn_test_df))\n",
    "\n",
    "dflrcsq = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dflrcsq['lrcsq'] = (dflrcsq['lrcsq_parnn'] + dflrcsq['lrcsq_lstm'] + dflrcsq['lrcsq_bilstm'] \n",
    "                  + dflrcsq['lrcsq_cgcn'] + dflrcsq['lrcsq_gcn']) / 5\n",
    "\n",
    "dflrcsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrcsq = dflrcsq.sort_values(by=['lrcsq'], ascending=False)\n",
    "sorted_dflrcsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_lrcsq = []\n",
    "for i in range(len(sorted_dflrcsq)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrcsq[:i]))\n",
    "    re_freq_lrcsq.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_lrcsq))\n",
    "\n",
    "re_per_lrcsq = []\n",
    "for i in range(1, len(sorted_dflrcsq)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dflrcsq[:i])) / i\n",
    "    re_per_lrcsq.append(n_dp)\n",
    "    \n",
    "print(len(re_per_lrcsq))\n",
    "\n",
    "re_intersection_lrcsq = []\n",
    "for i in range(5,len(sorted_dflrcsq)):\n",
    "    s = list(retacred_test.merge(sorted_dflrcsq[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    re_intersection_lrcsq.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_intersection_lrcsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retacred_test.merge(sorted_dflrcsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_ens = []\n",
    "n_dp = 0\n",
    "step_size = 200\n",
    "prev_s1 = prev_s2 = list()\n",
    "r = set(list(retacred_test['sentence_id']))\n",
    "for i in range(len(sorted_dfconf)):\n",
    "    s1 = s2 = list()\n",
    "    if i % step_size == 0:\n",
    "        j = int(i/2)\n",
    "        s1 = prev_s1[:]\n",
    "        s1 += list(sorted_dfrl[len(prev_s1):j]['sentence_id'])\n",
    "        print(len(prev_s1), len(s1))\n",
    "        k=0\n",
    "        while len(set(s1+s2)) < 2*j and (j+k < len(sorted_dfconf)):\n",
    "            s2 = prev_s2[:]\n",
    "            s2 += list(sorted_dfconf[len(prev_s2):j+k]['sentence_id'])\n",
    "            print(i, j, len(prev_s2), k, \"s1 + s2  :: \", len(set(s1+s2)))\n",
    "            k += 1\n",
    "        s = set(s1 + s2)\n",
    "        n_dp= len(r.intersection(s))\n",
    "        print(i, n_dp)\n",
    "        prev_s1 = s1\n",
    "        prev_s2 = s2\n",
    "        print(len(prev_s1), len(prev_s2))\n",
    "        print(\"\")\n",
    "    re_freq_ens.append(n_dp)\n",
    "    \n",
    "    \n",
    "print(len(re_freq_ens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnn_test_df = parnn_test.sort_values(by=['confidence'], ascending=False).reset_index(drop=True)\n",
    "parnn_test_df['conf_percentile'] = (len(parnn_test_df) - np.array(parnn_test_df.index) + 1) / len(parnn_test_df)\n",
    "parnn_test_df = parnn_test_df.sort_values(by=['lratio'], ascending=False).reset_index(drop=True)\n",
    "parnn_test_df['lratio_percentile'] = (len(parnn_test_df) - np.array(parnn_test_df.index) + 1) / len(parnn_test_df)\n",
    "parnn_test_df['percentile'] = parnn_test_df.lratio_percentile * np.power(parnn_test_df.conf_percentile, 2)\n",
    "parnn_test_df = parnn_test_df.loc[:,['sentence_id', 'percentile']]\n",
    "parnn_test_df = parnn_test_df.rename(columns={'percentile':'percentile_parnn'})\n",
    "\n",
    "lstm_test_df = lstm_test.sort_values(by=['confidence'], ascending=False).reset_index(drop=True)\n",
    "lstm_test_df['conf_percentile'] = (len(lstm_test_df) - np.array(lstm_test_df.index) + 1) / len(lstm_test_df)\n",
    "lstm_test_df = lstm_test_df.sort_values(by=['lratio'], ascending=False).reset_index(drop=True)\n",
    "lstm_test_df['lratio_percentile'] = (len(lstm_test_df) - np.array(lstm_test_df.index) + 1) / len(lstm_test_df)\n",
    "lstm_test_df['percentile'] = lstm_test_df.lratio_percentile * np.power(lstm_test_df.conf_percentile, 2)\n",
    "lstm_test_df = lstm_test_df.loc[:,['sentence_id', 'percentile']]\n",
    "lstm_test_df = lstm_test_df.rename(columns={'percentile':'percentile_lstm'})\n",
    "\n",
    "bilstm_test_df = bilstm_test.sort_values(by=['confidence'], ascending=False).reset_index(drop=True)\n",
    "bilstm_test_df['conf_percentile'] = (len(bilstm_test_df) - np.array(bilstm_test_df.index) + 1) / len(bilstm_test_df)\n",
    "bilstm_test_df = bilstm_test_df.sort_values(by=['lratio'], ascending=False).reset_index(drop=True)\n",
    "bilstm_test_df['lratio_percentile'] = (len(bilstm_test_df) - np.array(bilstm_test_df.index) + 1) / len(bilstm_test_df)\n",
    "bilstm_test_df['percentile'] = bilstm_test_df.lratio_percentile * np.power(bilstm_test_df.conf_percentile, 2)\n",
    "bilstm_test_df = bilstm_test_df.loc[:,['sentence_id', 'percentile']]\n",
    "bilstm_test_df = bilstm_test_df.rename(columns={'percentile':'percentile_bilstm'})\n",
    "\n",
    "cgcn_test_df = cgcn_test.sort_values(by=['confidence'], ascending=False).reset_index(drop=True)\n",
    "cgcn_test_df['conf_percentile'] = (len(cgcn_test_df) - np.array(cgcn_test_df.index) + 1) / len(cgcn_test_df)\n",
    "cgcn_test_df = cgcn_test_df.sort_values(by=['lratio'], ascending=False).reset_index(drop=True)\n",
    "cgcn_test_df['lratio_percentile'] = (len(cgcn_test_df) - np.array(cgcn_test_df.index) + 1) / len(cgcn_test_df)\n",
    "cgcn_test_df['percentile'] = cgcn_test_df.lratio_percentile * np.power(cgcn_test_df.conf_percentile, 2)\n",
    "cgcn_test_df = cgcn_test_df.loc[:,['sentence_id', 'percentile']]\n",
    "cgcn_test_df = cgcn_test_df.rename(columns={'percentile':'percentile_cgcn'})\n",
    "\n",
    "gcn_test_df = gcn_test.sort_values(by=['confidence'], ascending=False).reset_index(drop=True)\n",
    "gcn_test_df['conf_percentile'] = (len(gcn_test_df) - np.array(gcn_test_df.index) + 1) / len(gcn_test_df)\n",
    "gcn_test_df = gcn_test_df.sort_values(by=['lratio'], ascending=False).reset_index(drop=True)\n",
    "gcn_test_df['lratio_percentile'] = (len(gcn_test_df) - np.array(gcn_test_df.index) + 1) / len(gcn_test_df)\n",
    "gcn_test_df['percentile'] = gcn_test_df.lratio_percentile * np.power(gcn_test_df.conf_percentile, 2)\n",
    "gcn_test_df = gcn_test_df.loc[:,['sentence_id', 'percentile']]\n",
    "gcn_test_df = gcn_test_df.rename(columns={'percentile':'percentile_gcn'})\n",
    "\n",
    "dfpercentile = parnn_test_df.merge(lstm_test_df).merge(bilstm_test_df).merge(cgcn_test_df).merge(gcn_test_df)\n",
    "dfpercentile['percentile'] = (dfpercentile['percentile_parnn'] + dfpercentile['percentile_lstm'] + dfpercentile['percentile_bilstm'] \n",
    "                  + dfpercentile['percentile_cgcn'] + dfpercentile['percentile_gcn']) / 5\n",
    "\n",
    "dfpercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfpercentile = dfpercentile.sort_values(by=['percentile'], ascending=False)\n",
    "sorted_dfpercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_freq_percentile = []\n",
    "for i in range(len(sorted_dfpercentile)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfpercentile[:i]))\n",
    "    re_freq_percentile.append(n_dp)\n",
    "    \n",
    "print(len(re_freq_percentile))\n",
    "\n",
    "re_per_percentile = []\n",
    "for i in range(1, len(sorted_dfpercentile)):\n",
    "    n_dp = len(retacred_test.merge(sorted_dfpercentile[:i])) / i\n",
    "    re_per_percentile.append(n_dp)\n",
    "    \n",
    "print(len(re_per_percentile))\n",
    "\n",
    "re_intersection_percentile = []\n",
    "for i in range(5,len(sorted_dfpercentile)):\n",
    "    s = list(retacred_test.merge(sorted_dfpercentile[:i])['sentence_id'])\n",
    "    c = list(retacred_test.merge(sorted_dfconf[:i])['sentence_id'])\n",
    "    re_intersection_percentile.append(jaccard_similarity(s, c)*100)\n",
    "    \n",
    "print(len(re_intersection_percentile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'Confidence':re_freq_c, 'Ratio':re_freq_rl, 'Ratio*Conf':re_freq_lrc, 'Ratio*Square(Conf)':re_freq_lrcs, \n",
    "                     'Ratio*Sqrt(Conf)':re_freq_lrcsq, 'PERCENTILE':re_freq_percentile})\n",
    "plt.figure(figsize=(20,16))\n",
    "ax1 = plt.subplot(2,2,1)\n",
    "ax1 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Number of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "# plt.yticks(np.arange(0, 700, 70))\n",
    "\n",
    "\n",
    "freq = pd.DataFrame({'Confidence':re_freq_c, 'Ratio':re_freq_rl, 'Ratio*Conf':re_freq_lrc, 'Ratio*Square(Conf)':re_freq_lrcs, \n",
    "                     'Ratio*Sqrt(Conf)':re_freq_lrcsq, 'PERCENTILE':re_freq_percentile})\n",
    "freq = freq / 3936 * 100\n",
    "ax2 = plt.subplot(2,2,2)\n",
    "ax2 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Percentage of sentences common with LC', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax2.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "freq = pd.DataFrame({'Confidence':re_per_c, 'Ratio':re_per_rl, 'Ratio*Conf':re_per_lrc, 'Ratio*Square(Conf)':re_per_lrcs, \n",
    "                     'Ratio*Sqrt(Conf)':re_per_lrcsq, 'PERCENTILE':re_per_percentile})\n",
    "freq = freq*100\n",
    "ax3 = plt.subplot(2,2,3)\n",
    "ax3 = sns.lineplot(data=freq, dashes=False)\n",
    "plt.ylabel('Ratio of sentences common with LC and Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax3.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "intersection = pd.DataFrame({'Confidence Vs Confidence':re_intersection_c, 'Ratio Vs Confidence':re_intersection_rl, 'Ratio*Conf Vs Confidence':re_intersection_lrc,\n",
    "                             'Ratio*Square(Conf) Vs Confidence':re_intersection_lrcs, 'Ratio*Sqrt(Conf) Vs Confidence':re_intersection_lrcsq, \n",
    "                             'PERCENTILE':re_intersection_percentile})\n",
    "ax4 = plt.subplot(2,2,4)\n",
    "ax4 = sns.lineplot(data=intersection, dashes=False)\n",
    "plt.ylabel('Jaccard Similarity', fontsize='large', fontweight='bold')\n",
    "plt.xlabel('Reannotation Budget', fontsize='large', fontweight='bold')\n",
    "plt.title('Test Dataset', fontsize='large', fontweight='bold')\n",
    "ax4.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=100))\n",
    "\n",
    "# plt.savefig('ratio-confidence-test-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrcsq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrcs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dflrc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dfrl.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank = []\n",
    "# for sid in tt_dict:\n",
    "#     cnt = 0\n",
    "# #     print(sid, tt_dict[sid])\n",
    "#     if sid in list(parnn_test.sentence_id):\n",
    "#         if tt_dict[sid] == int(parnn_test[parnn_test.sentence_id == sid]['prediction']):\n",
    "#             cnt+=1\n",
    "#             print(sid, tt_dict[sid], int(parnn_test[parnn_test.sentence_id == sid]['prediction']), cnt)\n",
    "    \n",
    "#     if sid in list(lstm_test.sentence_id):\n",
    "#         if tt_dict[sid] == int(lstm_test[lstm_test.sentence_id == sid]['prediction']):\n",
    "#             cnt+=1\n",
    "#             print(sid, tt_dict[sid], int(lstm_test[lstm_test.sentence_id == sid]['prediction']), cnt)\n",
    "            \n",
    "#     if sid in list(bilstm_test.sentence_id):\n",
    "#         if tt_dict[sid] == int(bilstm_test[bilstm_test.sentence_id == sid]['prediction']):\n",
    "#             cnt+=1\n",
    "#             print(sid, tt_dict[sid], int(bilstm_test[bilstm_test.sentence_id == sid]['prediction']), cnt)\n",
    "            \n",
    "#     if sid in list(cgcn_test.sentence_id):\n",
    "#         if tt_dict[sid] == int(cgcn_test[cgcn_test.sentence_id == sid]['prediction']):\n",
    "#             cnt+=1\n",
    "#             print(sid, tt_dict[sid], int(cgcn_test[cgcn_test.sentence_id == sid]['prediction']), cnt)\n",
    "    \n",
    "#     if sid in list(gcn_test.sentence_id):\n",
    "#         if tt_dict[sid] == int(gcn_test[gcn_test.sentence_id == sid]['prediction']):\n",
    "#             cnt+=1\n",
    "#             print(sid, tt_dict[sid], int(gcn_test[gcn_test.sentence_id == sid]['prediction']), cnt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train = json.load(open('./../dataset/tacred/json/Re-TACRED/train_id2label.json'))\n",
    "print(\"Total Number of instances in ReTACRED-Reduced train set  :: {}\".format(len(re_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
